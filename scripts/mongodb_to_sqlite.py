import os
import json
import sys  # æ–°å¢ç³»ç»Ÿæ¨¡å—å¯¼å…¥

# import time
import pickle
from pathlib import Path

sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
from typing import Dict, Any, List, Optional, Type
from dataclasses import dataclass, field
from datetime import datetime
from pymongo import MongoClient
from pymongo.errors import ConnectionFailure
from peewee import Model, Field, IntegrityError

# Rich è¿›åº¦æ¡å’Œæ˜¾ç¤ºç»„ä»¶
from rich.console import Console
from rich.progress import (
    Progress,
    TextColumn,
    BarColumn,
    TaskProgressColumn,
    TimeRemainingColumn,
    TimeElapsedColumn,
    SpinnerColumn,
)
from rich.table import Table
from rich.panel import Panel
# from rich.text import Text

from src.common.database.database import db
from src.common.database.database_model import (
    ChatStreams,
    LLMUsage,
    Emoji,
    Messages,
    Images,
    ImageDescriptions,
    PersonInfo,
    Knowledges,
    ThinkingLog,
    GraphNodes,
    GraphEdges,
)
from src.common.logger_manager import get_logger

logger = get_logger("mongodb_to_sqlite")

ROOT_PATH = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))


@dataclass
class MigrationConfig:
    """è¿ç§»é…ç½®ç±»"""

    mongo_collection: str
    target_model: Type[Model]
    field_mapping: Dict[str, str]
    batch_size: int = 500
    enable_validation: bool = True
    skip_duplicates: bool = True
    unique_fields: List[str] = field(default_factory=list)  # ç”¨äºé‡å¤æ£€æŸ¥çš„å­—æ®µ


# æ•°æ®éªŒè¯ç›¸å…³ç±»å·²ç§»é™¤ - ç”¨æˆ·è¦æ±‚ä¸è¦æ•°æ®éªŒè¯


@dataclass
class MigrationCheckpoint:
    """è¿ç§»æ–­ç‚¹æ•°æ®"""

    collection_name: str
    processed_count: int
    last_processed_id: Any
    timestamp: datetime
    batch_errors: List[Dict[str, Any]] = field(default_factory=list)


@dataclass
class MigrationStats:
    """è¿ç§»ç»Ÿè®¡ä¿¡æ¯"""

    total_documents: int = 0
    processed_count: int = 0
    success_count: int = 0
    error_count: int = 0
    skipped_count: int = 0
    duplicate_count: int = 0
    validation_errors: int = 0
    batch_insert_count: int = 0
    errors: List[Dict[str, Any]] = field(default_factory=list)
    start_time: Optional[datetime] = None
    end_time: Optional[datetime] = None

    def add_error(self, doc_id: Any, error: str, doc_data: Optional[Dict] = None):
        """æ·»åŠ é”™è¯¯è®°å½•"""
        self.errors.append(
            {"doc_id": str(doc_id), "error": error, "timestamp": datetime.now().isoformat(), "doc_data": doc_data}
        )
        self.error_count += 1

    def add_validation_error(self, doc_id: Any, field: str, error: str):
        """æ·»åŠ éªŒè¯é”™è¯¯"""
        self.add_error(doc_id, f"éªŒè¯å¤±è´¥ - {field}: {error}")
        self.validation_errors += 1


class MongoToSQLiteMigrator:
    """MongoDBåˆ°SQLiteæ•°æ®è¿ç§»å™¨ - ä½¿ç”¨Peewee ORM"""

    def __init__(self, mongo_uri: Optional[str] = None, database_name: Optional[str] = None):
        self.database_name = database_name or os.getenv("DATABASE_NAME", "MegBot")
        self.mongo_uri = mongo_uri or self._build_mongo_uri()
        self.mongo_client: Optional[MongoClient] = None
        self.mongo_db = None

        # è¿ç§»é…ç½®
        self.migration_configs = self._initialize_migration_configs()

        # è¿›åº¦æ¡æ§åˆ¶å°
        self.console = Console()
        # æ£€æŸ¥ç‚¹ç›®å½•
        self.checkpoint_dir = Path(os.path.join(ROOT_PATH, "data", "checkpoints"))
        self.checkpoint_dir.mkdir(exist_ok=True)

        # éªŒè¯è§„åˆ™å·²ç¦ç”¨
        self.validation_rules = self._initialize_validation_rules()

    def _build_mongo_uri(self) -> str:
        """æ„å»ºMongoDBè¿æ¥URI"""
        if mongo_uri := os.getenv("MONGODB_URI"):
            return mongo_uri

        user = os.getenv("MONGODB_USER")
        password = os.getenv("MONGODB_PASS")
        host = os.getenv("MONGODB_HOST", "localhost")
        port = os.getenv("MONGODB_PORT", "27017")
        auth_source = os.getenv("MONGODB_AUTH_SOURCE", "admin")

        if user and password:
            return f"mongodb://{user}:{password}@{host}:{port}/{self.database_name}?authSource={auth_source}"
        else:
            return f"mongodb://{host}:{port}/{self.database_name}"

    def _initialize_migration_configs(self) -> List[MigrationConfig]:
        """åˆå§‹åŒ–è¿ç§»é…ç½®"""
        return [  # è¡¨æƒ…åŒ…è¿ç§»é…ç½®
            MigrationConfig(
                mongo_collection="emoji",
                target_model=Emoji,
                field_mapping={
                    "full_path": "full_path",
                    "format": "format",
                    "hash": "emoji_hash",
                    "description": "description",
                    "emotion": "emotion",
                    "usage_count": "usage_count",
                    "last_used_time": "last_used_time",
                    # record_timeå­—æ®µå°†åœ¨è½¬æ¢æ—¶è‡ªåŠ¨è®¾ç½®ä¸ºå½“å‰æ—¶é—´
                },
                enable_validation=False,  # ç¦ç”¨æ•°æ®éªŒè¯
                unique_fields=["full_path", "emoji_hash"],
            ),
            # èŠå¤©æµè¿ç§»é…ç½®
            MigrationConfig(
                mongo_collection="chat_streams",
                target_model=ChatStreams,
                field_mapping={
                    "stream_id": "stream_id",
                    "create_time": "create_time",
                    "group_info.platform": "group_platform",  # ç”±äºMongodbå¤„ç†ç§èŠæ—¶ä¼šè®©group_infoå€¼ä¸ºnullï¼Œè€Œæ–°çš„æ•°æ®åº“ä¸å…è®¸ä¸ºnullï¼Œæ‰€ä»¥ç§èŠèŠå¤©æµæ˜¯æ²¡æ³•è¿ç§»çš„ï¼Œç­‰æ›´æ–°å§ã€‚
                    "group_info.group_id": "group_id",  # åŒä¸Š
                    "group_info.group_name": "group_name",  # åŒä¸Š
                    "last_active_time": "last_active_time",
                    "platform": "platform",
                    "user_info.platform": "user_platform",
                    "user_info.user_id": "user_id",
                    "user_info.user_nickname": "user_nickname",
                    "user_info.user_cardname": "user_cardname",
                },
                enable_validation=False,  # ç¦ç”¨æ•°æ®éªŒè¯
                unique_fields=["stream_id"],
            ),
            # LLMä½¿ç”¨è®°å½•è¿ç§»é…ç½®
            MigrationConfig(
                mongo_collection="llm_usage",
                target_model=LLMUsage,
                field_mapping={
                    "model_name": "model_name",
                    "user_id": "user_id",
                    "request_type": "request_type",
                    "endpoint": "endpoint",
                    "prompt_tokens": "prompt_tokens",
                    "completion_tokens": "completion_tokens",
                    "total_tokens": "total_tokens",
                    "cost": "cost",
                    "status": "status",
                    "timestamp": "timestamp",
                },
                enable_validation=True,  # ç¦ç”¨æ•°æ®éªŒè¯"
                unique_fields=["user_id", "prompt_tokens", "completion_tokens", "total_tokens", "cost"],  # ç»„åˆå”¯ä¸€æ€§
            ),
            # æ¶ˆæ¯è¿ç§»é…ç½®
            MigrationConfig(
                mongo_collection="messages",
                target_model=Messages,
                field_mapping={
                    "message_id": "message_id",
                    "time": "time",
                    "chat_id": "chat_id",
                    "chat_info.stream_id": "chat_info_stream_id",
                    "chat_info.platform": "chat_info_platform",
                    "chat_info.user_info.platform": "chat_info_user_platform",
                    "chat_info.user_info.user_id": "chat_info_user_id",
                    "chat_info.user_info.user_nickname": "chat_info_user_nickname",
                    "chat_info.user_info.user_cardname": "chat_info_user_cardname",
                    "chat_info.group_info.platform": "chat_info_group_platform",
                    "chat_info.group_info.group_id": "chat_info_group_id",
                    "chat_info.group_info.group_name": "chat_info_group_name",
                    "chat_info.create_time": "chat_info_create_time",
                    "chat_info.last_active_time": "chat_info_last_active_time",
                    "user_info.platform": "user_platform",
                    "user_info.user_id": "user_id",
                    "user_info.user_nickname": "user_nickname",
                    "user_info.user_cardname": "user_cardname",
                    "processed_plain_text": "processed_plain_text",
                    "detailed_plain_text": "detailed_plain_text",
                    "memorized_times": "memorized_times",
                },
                enable_validation=False,  # ç¦ç”¨æ•°æ®éªŒè¯
                unique_fields=["message_id"],
            ),
            # å›¾ç‰‡è¿ç§»é…ç½®
            MigrationConfig(
                mongo_collection="images",
                target_model=Images,
                field_mapping={
                    "hash": "emoji_hash",
                    "description": "description",
                    "path": "path",
                    "timestamp": "timestamp",
                    "type": "type",
                },
                unique_fields=["path"],
            ),
            # å›¾ç‰‡æè¿°è¿ç§»é…ç½®
            MigrationConfig(
                mongo_collection="image_descriptions",
                target_model=ImageDescriptions,
                field_mapping={
                    "type": "type",
                    "hash": "image_description_hash",
                    "description": "description",
                    "timestamp": "timestamp",
                },
                unique_fields=["image_description_hash", "type"],
            ),
            # ä¸ªäººä¿¡æ¯è¿ç§»é…ç½®
            MigrationConfig(
                mongo_collection="person_info",
                target_model=PersonInfo,
                field_mapping={
                    "person_id": "person_id",
                    "person_name": "person_name",
                    "name_reason": "name_reason",
                    "platform": "platform",
                    "user_id": "user_id",
                    "nickname": "nickname",
                    "relationship_value": "relationship_value",
                    "konw_time": "know_time",
                    "msg_interval": "msg_interval",
                    "msg_interval_list": "msg_interval_list",
                },
                unique_fields=["person_id"],
            ),
            # çŸ¥è¯†åº“è¿ç§»é…ç½®
            MigrationConfig(
                mongo_collection="knowledges",
                target_model=Knowledges,
                field_mapping={"content": "content", "embedding": "embedding"},
                unique_fields=["content"],  # å‡è®¾å†…å®¹å”¯ä¸€
            ),
            # æ€è€ƒæ—¥å¿—è¿ç§»é…ç½®
            MigrationConfig(
                mongo_collection="thinking_log",
                target_model=ThinkingLog,
                field_mapping={
                    "chat_id": "chat_id",
                    "trigger_text": "trigger_text",
                    "response_text": "response_text",
                    "trigger_info": "trigger_info_json",
                    "response_info": "response_info_json",
                    "timing_results": "timing_results_json",
                    "chat_history": "chat_history_json",
                    "chat_history_in_thinking": "chat_history_in_thinking_json",
                    "chat_history_after_response": "chat_history_after_response_json",
                    "heartflow_data": "heartflow_data_json",
                    "reasoning_data": "reasoning_data_json",
                },
                unique_fields=["chat_id", "trigger_text"],
            ),
            # å›¾èŠ‚ç‚¹è¿ç§»é…ç½®
            MigrationConfig(
                mongo_collection="graph_data.nodes",
                target_model=GraphNodes,
                field_mapping={
                    "concept": "concept",
                    "memory_items": "memory_items",
                    "hash": "hash",
                    "created_time": "created_time",
                    "last_modified": "last_modified",
                },
                unique_fields=["concept"],
            ),
            # å›¾è¾¹è¿ç§»é…ç½®
            MigrationConfig(
                mongo_collection="graph_data.edges",
                target_model=GraphEdges,
                field_mapping={
                    "source": "source",
                    "target": "target",
                    "strength": "strength",
                    "hash": "hash",
                    "created_time": "created_time",
                    "last_modified": "last_modified",
                },
                unique_fields=["source", "target"],  # ç»„åˆå”¯ä¸€æ€§
            ),
        ]

    def _initialize_validation_rules(self) -> Dict[str, Any]:
        """æ•°æ®éªŒè¯å·²ç¦ç”¨ - è¿”å›ç©ºå­—å…¸"""
        return {}

    def connect_mongodb(self) -> bool:
        """è¿æ¥åˆ°MongoDB"""
        try:
            self.mongo_client = MongoClient(
                self.mongo_uri, serverSelectionTimeoutMS=5000, connectTimeoutMS=10000, maxPoolSize=10
            )

            # æµ‹è¯•è¿æ¥
            self.mongo_client.admin.command("ping")
            self.mongo_db = self.mongo_client[self.database_name]

            logger.info(f"æˆåŠŸè¿æ¥åˆ°MongoDB: {self.database_name}")
            return True

        except ConnectionFailure as e:
            logger.error(f"MongoDBè¿æ¥å¤±è´¥: {e}")
            return False
        except Exception as e:
            logger.error(f"MongoDBè¿æ¥å¼‚å¸¸: {e}")
            return False

    def disconnect_mongodb(self):
        """æ–­å¼€MongoDBè¿æ¥"""
        if self.mongo_client:
            self.mongo_client.close()
            logger.info("MongoDBè¿æ¥å·²å…³é—­")

    def _get_nested_value(self, document: Dict[str, Any], field_path: str) -> Any:
        """è·å–åµŒå¥—å­—æ®µçš„å€¼"""
        if "." not in field_path:
            return document.get(field_path)

        parts = field_path.split(".")
        value = document

        for part in parts:
            if isinstance(value, dict):
                value = value.get(part)
            else:
                return None

            if value is None:
                break

        return value

    def _convert_field_value(self, value: Any, target_field: Field) -> Any:
        """æ ¹æ®ç›®æ ‡å­—æ®µç±»å‹è½¬æ¢å€¼"""
        if value is None:
            return None

        field_type = target_field.__class__.__name__

        try:
            if target_field.name == "record_time" and field_type == "DateTimeField":
                return datetime.now()

            if field_type in ["CharField", "TextField"]:
                if isinstance(value, (list, dict)):
                    return json.dumps(value, ensure_ascii=False)
                return str(value) if value is not None else ""

            elif field_type == "IntegerField":
                if isinstance(value, str):
                    # å¤„ç†å­—ç¬¦ä¸²æ•°å­—
                    clean_value = value.strip()
                    if clean_value.replace(".", "").replace("-", "").isdigit():
                        return int(float(clean_value))
                    return 0
                return int(value) if value is not None else 0

            elif field_type in ["FloatField", "DoubleField"]:
                return float(value) if value is not None else 0.0

            elif field_type == "BooleanField":
                if isinstance(value, str):
                    return value.lower() in ("true", "1", "yes", "on")
                return bool(value)

            elif field_type == "DateTimeField":
                if isinstance(value, (int, float)):
                    return datetime.fromtimestamp(value)
                elif isinstance(value, str):
                    try:
                        # å°è¯•è§£æISOæ ¼å¼æ—¥æœŸ
                        return datetime.fromisoformat(value.replace("Z", "+00:00"))
                    except ValueError:
                        try:
                            # å°è¯•è§£ææ—¶é—´æˆ³å­—ç¬¦ä¸²
                            return datetime.fromtimestamp(float(value))
                        except ValueError:
                            return datetime.now()
                return datetime.now()

            return value

        except (ValueError, TypeError) as e:
            logger.warning(f"å­—æ®µå€¼è½¬æ¢å¤±è´¥ ({field_type}): {value} -> {e}")
            return self._get_default_value_for_field(target_field)

    def _get_default_value_for_field(self, field: Field) -> Any:
        """è·å–å­—æ®µçš„é»˜è®¤å€¼"""
        field_type = field.__class__.__name__

        if hasattr(field, "default") and field.default is not None:
            return field.default

        if field.null:
            return None

        # æ ¹æ®å­—æ®µç±»å‹è¿”å›é»˜è®¤å€¼
        if field_type in ["CharField", "TextField"]:
            return ""
        elif field_type == "IntegerField":
            return 0
        elif field_type in ["FloatField", "DoubleField"]:
            return 0.0
        elif field_type == "BooleanField":
            return False
        elif field_type == "DateTimeField":
            return datetime.now()

        return None

    def _validate_data(self, collection_name: str, data: Dict[str, Any], doc_id: Any, stats: MigrationStats) -> bool:
        """æ•°æ®éªŒè¯å·²ç¦ç”¨ - å§‹ç»ˆè¿”å›True"""
        return True

    def _save_checkpoint(self, collection_name: str, processed_count: int, last_id: Any):
        """ä¿å­˜è¿ç§»æ–­ç‚¹"""
        checkpoint = MigrationCheckpoint(
            collection_name=collection_name,
            processed_count=processed_count,
            last_processed_id=last_id,
            timestamp=datetime.now(),
        )

        checkpoint_file = self.checkpoint_dir / f"{collection_name}_checkpoint.pkl"
        try:
            with open(checkpoint_file, "wb") as f:
                pickle.dump(checkpoint, f)
        except Exception as e:
            logger.warning(f"ä¿å­˜æ–­ç‚¹å¤±è´¥: {e}")

    def _load_checkpoint(self, collection_name: str) -> Optional[MigrationCheckpoint]:
        """åŠ è½½è¿ç§»æ–­ç‚¹"""
        checkpoint_file = self.checkpoint_dir / f"{collection_name}_checkpoint.pkl"
        if not checkpoint_file.exists():
            return None

        try:
            with open(checkpoint_file, "rb") as f:
                return pickle.load(f)
        except Exception as e:
            logger.warning(f"åŠ è½½æ–­ç‚¹å¤±è´¥: {e}")
            return None

    def _batch_insert(self, model: Type[Model], data_list: List[Dict[str, Any]]) -> int:
        """æ‰¹é‡æ’å…¥æ•°æ®"""
        if not data_list:
            return 0

        success_count = 0
        try:
            with db.atomic():
                # åˆ†æ‰¹æ’å…¥ï¼Œé¿å…SQLè¯­å¥è¿‡é•¿
                batch_size = 100
                for i in range(0, len(data_list), batch_size):
                    batch = data_list[i : i + batch_size]
                    model.insert_many(batch).execute()
                    success_count += len(batch)
        except Exception as e:
            logger.error(f"æ‰¹é‡æ’å…¥å¤±è´¥: {e}")
            # å¦‚æœæ‰¹é‡æ’å…¥å¤±è´¥ï¼Œå°è¯•é€ä¸ªæ’å…¥
            for data in data_list:
                try:
                    model.create(**data)
                    success_count += 1
                except Exception:
                    pass  # å¿½ç•¥å•ä¸ªæ’å…¥å¤±è´¥

        return success_count

    def _check_duplicate_by_unique_fields(
        self, model: Type[Model], data: Dict[str, Any], unique_fields: List[str]
    ) -> bool:
        """æ ¹æ®å”¯ä¸€å­—æ®µæ£€æŸ¥é‡å¤"""
        if not unique_fields:
            return False

        try:
            query = model.select()
            for field_name in unique_fields:
                if field_name in data and data[field_name] is not None:
                    field_obj = getattr(model, field_name)
                    query = query.where(field_obj == data[field_name])

            return query.exists()
        except Exception as e:
            logger.debug(f"é‡å¤æ£€æŸ¥å¤±è´¥: {e}")
            return False

    def _create_model_instance(self, model: Type[Model], data: Dict[str, Any]) -> Optional[Model]:
        """ä½¿ç”¨ORMåˆ›å»ºæ¨¡å‹å®ä¾‹"""
        try:
            # è¿‡æ»¤æ‰ä¸å­˜åœ¨çš„å­—æ®µ
            valid_data = {}
            for field_name, value in data.items():
                if hasattr(model, field_name):
                    valid_data[field_name] = value
                else:
                    logger.debug(f"è·³è¿‡æœªçŸ¥å­—æ®µ: {field_name}")

            # åˆ›å»ºå®ä¾‹
            instance = model.create(**valid_data)
            return instance

        except IntegrityError as e:
            # å¤„ç†å”¯ä¸€çº¦æŸå†²çªç­‰å®Œæ•´æ€§é”™è¯¯
            logger.debug(f"å®Œæ•´æ€§çº¦æŸå†²çª: {e}")
            return None
        except Exception as e:
            logger.error(f"åˆ›å»ºæ¨¡å‹å®ä¾‹å¤±è´¥: {e}")
            return None

    def migrate_collection(self, config: MigrationConfig) -> MigrationStats:
        """è¿ç§»å•ä¸ªé›†åˆ - ä½¿ç”¨ä¼˜åŒ–çš„æ‰¹é‡æ’å…¥å’Œè¿›åº¦æ¡"""
        stats = MigrationStats()
        stats.start_time = datetime.now()

        # æ£€æŸ¥æ˜¯å¦æœ‰æ–­ç‚¹
        checkpoint = self._load_checkpoint(config.mongo_collection)
        start_from_id = checkpoint.last_processed_id if checkpoint else None
        if checkpoint:
            stats.processed_count = checkpoint.processed_count
            logger.info(f"ä»æ–­ç‚¹æ¢å¤: å·²å¤„ç† {checkpoint.processed_count} æ¡è®°å½•")

        logger.info(f"å¼€å§‹è¿ç§»: {config.mongo_collection} -> {config.target_model._meta.table_name}")

        try:
            # è·å–MongoDBé›†åˆ
            mongo_collection = self.mongo_db[config.mongo_collection]

            # æ„å»ºæŸ¥è¯¢æ¡ä»¶ï¼ˆç”¨äºæ–­ç‚¹æ¢å¤ï¼‰
            query = {}
            if start_from_id:
                query = {"_id": {"$gt": start_from_id}}

            stats.total_documents = mongo_collection.count_documents(query)

            if stats.total_documents == 0:
                logger.warning(f"é›†åˆ {config.mongo_collection} ä¸ºç©ºï¼Œè·³è¿‡è¿ç§»")
                return stats

            logger.info(f"å¾…è¿ç§»æ–‡æ¡£æ•°é‡: {stats.total_documents}")

            # åˆ›å»ºRichè¿›åº¦æ¡
            with Progress(
                SpinnerColumn(),
                TextColumn("[progress.description]{task.description}"),
                BarColumn(),
                TaskProgressColumn(),
                TimeElapsedColumn(),
                TimeRemainingColumn(),
                console=self.console,
                refresh_per_second=10,
            ) as progress:
                task = progress.add_task(f"è¿ç§» {config.mongo_collection}", total=stats.total_documents)
                # æ‰¹é‡å¤„ç†æ•°æ®
                batch_data = []
                batch_count = 0
                last_processed_id = None

                for mongo_doc in mongo_collection.find(query).batch_size(config.batch_size):
                    try:
                        doc_id = mongo_doc.get("_id", "unknown")
                        last_processed_id = doc_id

                        # æ„å»ºç›®æ ‡æ•°æ®
                        target_data = {}
                        for mongo_field, sqlite_field in config.field_mapping.items():
                            value = self._get_nested_value(mongo_doc, mongo_field)

                            # è·å–ç›®æ ‡å­—æ®µå¯¹è±¡å¹¶è½¬æ¢ç±»å‹
                            if hasattr(config.target_model, sqlite_field):
                                field_obj = getattr(config.target_model, sqlite_field)
                                converted_value = self._convert_field_value(value, field_obj)
                                target_data[sqlite_field] = converted_value

                        # æ•°æ®éªŒè¯å·²ç¦ç”¨
                        # if config.enable_validation:
                        #     if not self._validate_data(config.mongo_collection, target_data, doc_id, stats):
                        #         stats.skipped_count += 1
                        #         continue

                        # é‡å¤æ£€æŸ¥
                        if config.skip_duplicates and self._check_duplicate_by_unique_fields(
                            config.target_model, target_data, config.unique_fields
                        ):
                            stats.duplicate_count += 1
                            stats.skipped_count += 1
                            logger.debug(f"è·³è¿‡é‡å¤è®°å½•: {doc_id}")
                            continue

                        # æ·»åŠ åˆ°æ‰¹é‡æ•°æ®
                        batch_data.append(target_data)
                        stats.processed_count += 1

                        # æ‰§è¡Œæ‰¹é‡æ’å…¥
                        if len(batch_data) >= config.batch_size:
                            success_count = self._batch_insert(config.target_model, batch_data)
                            stats.success_count += success_count
                            stats.batch_insert_count += 1

                            # ä¿å­˜æ–­ç‚¹
                            self._save_checkpoint(config.mongo_collection, stats.processed_count, last_processed_id)

                            batch_data.clear()
                            batch_count += 1

                            # æ›´æ–°è¿›åº¦æ¡
                            progress.update(task, advance=config.batch_size)

                    except Exception as e:
                        doc_id = mongo_doc.get("_id", "unknown")
                        stats.add_error(doc_id, f"å¤„ç†æ–‡æ¡£å¼‚å¸¸: {e}", mongo_doc)
                        logger.error(f"å¤„ç†æ–‡æ¡£å¤±è´¥ (ID: {doc_id}): {e}")

                # å¤„ç†å‰©ä½™çš„æ‰¹é‡æ•°æ®
                if batch_data:
                    success_count = self._batch_insert(config.target_model, batch_data)
                    stats.success_count += success_count
                    stats.batch_insert_count += 1
                    progress.update(task, advance=len(batch_data))

                # å®Œæˆè¿›åº¦æ¡
                progress.update(task, completed=stats.total_documents)

            stats.end_time = datetime.now()
            duration = stats.end_time - stats.start_time

            logger.info(
                f"è¿ç§»å®Œæˆ: {config.mongo_collection} -> {config.target_model._meta.table_name}\n"
                f"æ€»è®¡: {stats.total_documents}, æˆåŠŸ: {stats.success_count}, "
                f"é”™è¯¯: {stats.error_count}, è·³è¿‡: {stats.skipped_count}, é‡å¤: {stats.duplicate_count}\n"
                f"è€—æ—¶: {duration.total_seconds():.2f}ç§’, æ‰¹é‡æ’å…¥æ¬¡æ•°: {stats.batch_insert_count}"
            )

            # æ¸…ç†æ–­ç‚¹æ–‡ä»¶
            checkpoint_file = self.checkpoint_dir / f"{config.mongo_collection}_checkpoint.pkl"
            if checkpoint_file.exists():
                checkpoint_file.unlink()

        except Exception as e:
            logger.error(f"è¿ç§»é›†åˆ {config.mongo_collection} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            stats.add_error("collection_error", str(e))

        return stats

    def migrate_all(self) -> Dict[str, MigrationStats]:
        """æ‰§è¡Œæ‰€æœ‰è¿ç§»ä»»åŠ¡"""
        logger.info("å¼€å§‹æ‰§è¡Œæ•°æ®åº“è¿ç§»...")

        if not self.connect_mongodb():
            logger.error("æ— æ³•è¿æ¥åˆ°MongoDBï¼Œè¿ç§»ç»ˆæ­¢")
            return {}

        all_stats = {}

        try:
            # åˆ›å»ºæ€»ä½“è¿›åº¦è¡¨æ ¼
            total_collections = len(self.migration_configs)
            self.console.print(
                Panel(
                    f"[bold blue]MongoDB åˆ° SQLite æ•°æ®è¿ç§»[/bold blue]\n"
                    f"[yellow]æ€»é›†åˆæ•°: {total_collections}[/yellow]",
                    title="è¿ç§»å¼€å§‹",
                    expand=False,
                )
            )
            for idx, config in enumerate(self.migration_configs, 1):
                self.console.print(
                    f"\n[bold green]æ­£åœ¨å¤„ç†é›†åˆ {idx}/{total_collections}: {config.mongo_collection}[/bold green]"
                )
                stats = self.migrate_collection(config)
                all_stats[config.mongo_collection] = stats

                # æ˜¾ç¤ºå•ä¸ªé›†åˆçš„å¿«é€Ÿç»Ÿè®¡
                if stats.processed_count > 0:
                    success_rate = stats.success_count / stats.processed_count * 100
                    if success_rate >= 95:
                        status_emoji = "âœ…"
                        status_color = "bright_green"
                    elif success_rate >= 80:
                        status_emoji = "âš ï¸"
                        status_color = "yellow"
                    else:
                        status_emoji = "âŒ"
                        status_color = "red"

                    self.console.print(
                        f"   {status_emoji} [{status_color}]å®Œæˆ: {stats.success_count}/{stats.processed_count} "
                        f"({success_rate:.1f}%) é”™è¯¯: {stats.error_count}[/{status_color}]"
                    )

                # é”™è¯¯ç‡æ£€æŸ¥
                if stats.processed_count > 0:
                    error_rate = stats.error_count / stats.processed_count
                    if error_rate > 0.1:  # é”™è¯¯ç‡è¶…è¿‡10%
                        self.console.print(
                            f"   [red]âš ï¸  è­¦å‘Š: é”™è¯¯ç‡è¾ƒé«˜ {error_rate:.1%} "
                            f"({stats.error_count}/{stats.processed_count})[/red]"
                        )

        finally:
            self.disconnect_mongodb()

        self._print_migration_summary(all_stats)
        return all_stats

    def _print_migration_summary(self, all_stats: Dict[str, MigrationStats]):
        """ä½¿ç”¨Richæ‰“å°ç¾è§‚çš„è¿ç§»æ±‡æ€»ä¿¡æ¯"""
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        total_processed = sum(stats.processed_count for stats in all_stats.values())
        total_success = sum(stats.success_count for stats in all_stats.values())
        total_errors = sum(stats.error_count for stats in all_stats.values())
        total_skipped = sum(stats.skipped_count for stats in all_stats.values())
        total_duplicates = sum(stats.duplicate_count for stats in all_stats.values())
        total_validation_errors = sum(stats.validation_errors for stats in all_stats.values())
        total_batch_inserts = sum(stats.batch_insert_count for stats in all_stats.values())

        # è®¡ç®—æ€»è€—æ—¶
        total_duration_seconds = 0
        for stats in all_stats.values():
            if stats.start_time and stats.end_time:
                duration = stats.end_time - stats.start_time
                total_duration_seconds += duration.total_seconds()

        # åˆ›å»ºè¯¦ç»†ç»Ÿè®¡è¡¨æ ¼
        table = Table(title="[bold blue]æ•°æ®è¿ç§»æ±‡æ€»æŠ¥å‘Š[/bold blue]", show_header=True, header_style="bold magenta")
        table.add_column("é›†åˆåç§°", style="cyan", width=20)
        table.add_column("æ–‡æ¡£æ€»æ•°", justify="right", style="blue")
        table.add_column("å¤„ç†æ•°é‡", justify="right", style="green")
        table.add_column("æˆåŠŸæ•°é‡", justify="right", style="green")
        table.add_column("é”™è¯¯æ•°é‡", justify="right", style="red")
        table.add_column("è·³è¿‡æ•°é‡", justify="right", style="yellow")
        table.add_column("é‡å¤æ•°é‡", justify="right", style="bright_yellow")
        table.add_column("éªŒè¯é”™è¯¯", justify="right", style="red")
        table.add_column("æ‰¹æ¬¡æ•°", justify="right", style="purple")
        table.add_column("æˆåŠŸç‡", justify="right", style="bright_green")
        table.add_column("è€—æ—¶(ç§’)", justify="right", style="blue")

        for collection_name, stats in all_stats.items():
            success_rate = (stats.success_count / stats.processed_count * 100) if stats.processed_count > 0 else 0
            duration = 0
            if stats.start_time and stats.end_time:
                duration = (stats.end_time - stats.start_time).total_seconds()

            # æ ¹æ®æˆåŠŸç‡è®¾ç½®é¢œè‰²
            if success_rate >= 95:
                success_rate_style = "[bright_green]"
            elif success_rate >= 80:
                success_rate_style = "[yellow]"
            else:
                success_rate_style = "[red]"

            table.add_row(
                collection_name,
                str(stats.total_documents),
                str(stats.processed_count),
                str(stats.success_count),
                f"[red]{stats.error_count}[/red]" if stats.error_count > 0 else "0",
                f"[yellow]{stats.skipped_count}[/yellow]" if stats.skipped_count > 0 else "0",
                f"[bright_yellow]{stats.duplicate_count}[/bright_yellow]" if stats.duplicate_count > 0 else "0",
                f"[red]{stats.validation_errors}[/red]" if stats.validation_errors > 0 else "0",
                str(stats.batch_insert_count),
                f"{success_rate_style}{success_rate:.1f}%[/{success_rate_style[1:]}",
                f"{duration:.2f}",
            )

        # æ·»åŠ æ€»è®¡è¡Œ
        total_success_rate = (total_success / total_processed * 100) if total_processed > 0 else 0
        if total_success_rate >= 95:
            total_rate_style = "[bright_green]"
        elif total_success_rate >= 80:
            total_rate_style = "[yellow]"
        else:
            total_rate_style = "[red]"

        table.add_section()
        table.add_row(
            "[bold]æ€»è®¡[/bold]",
            f"[bold]{sum(stats.total_documents for stats in all_stats.values())}[/bold]",
            f"[bold]{total_processed}[/bold]",
            f"[bold]{total_success}[/bold]",
            f"[bold red]{total_errors}[/bold red]" if total_errors > 0 else "[bold]0[/bold]",
            f"[bold yellow]{total_skipped}[/bold yellow]" if total_skipped > 0 else "[bold]0[/bold]",
            f"[bold bright_yellow]{total_duplicates}[/bold bright_yellow]"
            if total_duplicates > 0
            else "[bold]0[/bold]",
            f"[bold red]{total_validation_errors}[/bold red]" if total_validation_errors > 0 else "[bold]0[/bold]",
            f"[bold]{total_batch_inserts}[/bold]",
            f"[bold]{total_rate_style}{total_success_rate:.1f}%[/{total_rate_style[1:]}[/bold]",
            f"[bold]{total_duration_seconds:.2f}[/bold]",
        )

        self.console.print(table)

        # åˆ›å»ºçŠ¶æ€é¢æ¿
        status_items = []
        if total_errors > 0:
            status_items.append(f"[red]âš ï¸  å‘ç° {total_errors} ä¸ªé”™è¯¯ï¼Œè¯·æ£€æŸ¥æ—¥å¿—è¯¦æƒ…[/red]")

        if total_validation_errors > 0:
            status_items.append(f"[red]ğŸ” æ•°æ®éªŒè¯å¤±è´¥: {total_validation_errors} æ¡è®°å½•[/red]")

        if total_duplicates > 0:
            status_items.append(f"[yellow]ğŸ“‹ è·³è¿‡é‡å¤è®°å½•: {total_duplicates} æ¡[/yellow]")

        if total_success_rate >= 95:
            status_items.append(f"[bright_green]âœ… è¿ç§»æˆåŠŸç‡ä¼˜ç§€: {total_success_rate:.1f}%[/bright_green]")
        elif total_success_rate >= 80:
            status_items.append(f"[yellow]âš¡ è¿ç§»æˆåŠŸç‡è‰¯å¥½: {total_success_rate:.1f}%[/yellow]")
        else:
            status_items.append(f"[red]âŒ è¿ç§»æˆåŠŸç‡è¾ƒä½: {total_success_rate:.1f}%ï¼Œéœ€è¦æ£€æŸ¥[/red]")

        if status_items:
            status_panel = Panel(
                "\n".join(status_items), title="[bold yellow]è¿ç§»çŠ¶æ€æ€»ç»“[/bold yellow]", border_style="yellow"
            )
            self.console.print(status_panel)

        # æ€§èƒ½ç»Ÿè®¡é¢æ¿
        avg_speed = total_processed / total_duration_seconds if total_duration_seconds > 0 else 0
        performance_info = (
            f"[cyan]æ€»å¤„ç†æ—¶é—´:[/cyan] {total_duration_seconds:.2f} ç§’\n"
            f"[cyan]å¹³å‡å¤„ç†é€Ÿåº¦:[/cyan] {avg_speed:.1f} æ¡è®°å½•/ç§’\n"
            f"[cyan]æ‰¹é‡æ’å…¥ä¼˜åŒ–:[/cyan] æ‰§è¡Œäº† {total_batch_inserts} æ¬¡æ‰¹é‡æ“ä½œ"
        )

        performance_panel = Panel(performance_info, title="[bold green]æ€§èƒ½ç»Ÿè®¡[/bold green]", border_style="green")
        self.console.print(performance_panel)

    def add_migration_config(self, config: MigrationConfig):
        """æ·»åŠ æ–°çš„è¿ç§»é…ç½®"""
        self.migration_configs.append(config)

    def migrate_single_collection(self, collection_name: str) -> Optional[MigrationStats]:
        """è¿ç§»å•ä¸ªæŒ‡å®šçš„é›†åˆ"""
        config = next((c for c in self.migration_configs if c.mongo_collection == collection_name), None)
        if not config:
            logger.error(f"æœªæ‰¾åˆ°é›†åˆ {collection_name} çš„è¿ç§»é…ç½®")
            return None

        if not self.connect_mongodb():
            logger.error("æ— æ³•è¿æ¥åˆ°MongoDB")
            return None

        try:
            stats = self.migrate_collection(config)
            self._print_migration_summary({collection_name: stats})
            return stats
        finally:
            self.disconnect_mongodb()

    def export_error_report(self, all_stats: Dict[str, MigrationStats], filepath: str):
        """å¯¼å‡ºé”™è¯¯æŠ¥å‘Š"""
        error_report = {
            "timestamp": datetime.now().isoformat(),
            "summary": {
                collection: {
                    "total": stats.total_documents,
                    "processed": stats.processed_count,
                    "success": stats.success_count,
                    "errors": stats.error_count,
                    "skipped": stats.skipped_count,
                    "duplicates": stats.duplicate_count,
                }
                for collection, stats in all_stats.items()
            },
            "errors": {collection: stats.errors for collection, stats in all_stats.items() if stats.errors},
        }

        try:
            with open(filepath, "w", encoding="utf-8") as f:
                json.dump(error_report, f, ensure_ascii=False, indent=2)
            logger.info(f"é”™è¯¯æŠ¥å‘Šå·²å¯¼å‡ºåˆ°: {filepath}")
        except Exception as e:
            logger.error(f"å¯¼å‡ºé”™è¯¯æŠ¥å‘Šå¤±è´¥: {e}")


def main():
    """ä¸»ç¨‹åºå…¥å£"""
    migrator = MongoToSQLiteMigrator()

    # æ‰§è¡Œè¿ç§»
    migration_results = migrator.migrate_all()

    # å¯¼å‡ºé”™è¯¯æŠ¥å‘Šï¼ˆå¦‚æœæœ‰é”™è¯¯ï¼‰
    if any(stats.error_count > 0 for stats in migration_results.values()):
        error_report_path = f"migration_errors_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        migrator.export_error_report(migration_results, error_report_path)

    logger.info("æ•°æ®è¿ç§»å®Œæˆï¼")


if __name__ == "__main__":
    main()
