#!/usr/bin/env python3
"""
åŸºäºEmbeddingçš„å…´è¶£åº¦è®¡ç®—æµ‹è¯•è„šæœ¬
ä½¿ç”¨MaiBot-Coreçš„EmbeddingStoreè®¡ç®—å…´è¶£æè¿°ä¸ç›®æ ‡æ–‡æœ¬çš„å…³è”åº¦
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from typing import List, Dict, Tuple, Optional
import time
import json
import asyncio
from src.chat.knowledge.embedding_store import EmbeddingStore, cosine_similarity
from src.chat.knowledge.embedding_store import EMBEDDING_DATA_DIR_STR
from src.llm_models.utils_model import LLMRequest
from src.config.config import model_config


class InterestScorer:
    """åŸºäºEmbeddingçš„å…´è¶£åº¦è®¡ç®—å™¨"""
    
    def __init__(self, namespace: str = "interest_test"):
        """åˆå§‹åŒ–å…´è¶£åº¦è®¡ç®—å™¨"""
        self.embedding_store = EmbeddingStore(namespace, EMBEDDING_DATA_DIR_STR)
        
    async def get_embedding(self, text: str) -> Tuple[Optional[List[float]], float]:
        """è·å–æ–‡æœ¬çš„åµŒå…¥å‘é‡"""
        start_time = time.time()
        try:
            # ç›´æ¥ä½¿ç”¨å¼‚æ­¥æ–¹å¼è·å–åµŒå…¥
            from src.llm_models.utils_model import LLMRequest
            from src.config.config import model_config
            
            llm = LLMRequest(model_set=model_config.model_task_config.embedding, request_type="embedding")
            embedding, _ = await llm.get_embedding(text)
            
            end_time = time.time()
            elapsed = end_time - start_time
            
            if embedding and len(embedding) > 0:
                return embedding, elapsed
            return None, elapsed
        except Exception as e:
            print(f"è·å–åµŒå…¥å‘é‡å¤±è´¥: {e}")
            return None, 0.0
    
    async def calculate_similarity(self, text1: str, text2: str) -> Tuple[float, float, float]:
        """è®¡ç®—ä¸¤æ®µæ–‡æœ¬çš„ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œè¿”å›(ç›¸ä¼¼åº¦, æ–‡æœ¬1è€—æ—¶, æ–‡æœ¬2è€—æ—¶)"""
        emb1, time1 = await self.get_embedding(text1)
        emb2, time2 = await self.get_embedding(text2)
        
        if emb1 is None or emb2 is None:
            return 0.0, time1, time2
            
        return cosine_similarity(emb1, emb2), time1, time2
    
    async def calculate_interest_score(self, interest_text: str, target_text: str) -> Dict:
        """
        è®¡ç®—å…´è¶£åº¦åˆ†æ•°
        
        Args:
            interest_text: å…´è¶£æè¿°æ–‡æœ¬
            target_text: ç›®æ ‡æ–‡æœ¬
            
        Returns:
            åŒ…å«å„ç§åˆ†æ•°çš„å­—å…¸
        """
        # åªè®¡ç®—è¯­ä¹‰ç›¸ä¼¼åº¦ï¼ˆåµŒå…¥åˆ†æ•°ï¼‰
        semantic_score, interest_time, target_time = await self.calculate_similarity(interest_text, target_text)
        
        # ç›´æ¥ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦ä½œä¸ºæœ€ç»ˆåˆ†æ•°
        final_score = semantic_score
        
        return {
            "final_score": final_score,
            "semantic_score": semantic_score,
            "timing": {
                "interest_embedding_time": interest_time,
                "target_embedding_time": target_time,
                "total_time": interest_time + target_time
            }
        }
    
    async def batch_calculate(self, interest_text: str, target_texts: List[str]) -> List[Dict]:
        """æ‰¹é‡è®¡ç®—å…´è¶£åº¦"""
        results = []
        total_start_time = time.time()
        
        print(f"å¼€å§‹æ‰¹é‡è®¡ç®—å…´è¶£åº¦...")
        print(f"å…´è¶£æ–‡æœ¬: {interest_text}")
        print(f"ç›®æ ‡æ–‡æœ¬æ•°é‡: {len(target_texts)}")
        
        # è·å–å…´è¶£æ–‡æœ¬çš„åµŒå…¥å‘é‡ï¼ˆåªéœ€è¦ä¸€æ¬¡ï¼‰
        interest_embedding, interest_time = await self.get_embedding(interest_text)
        if interest_embedding is None:
            print("æ— æ³•è·å–å…´è¶£æ–‡æœ¬çš„åµŒå…¥å‘é‡")
            return []
        
        print(f"å…´è¶£æ–‡æœ¬åµŒå…¥è®¡ç®—è€—æ—¶: {interest_time:.3f}ç§’")
        
        total_target_time = 0.0
        
        for i, target_text in enumerate(target_texts):
            print(f"å¤„ç†ç¬¬ {i+1}/{len(target_texts)} ä¸ªæ–‡æœ¬...")
            
            # è·å–ç›®æ ‡æ–‡æœ¬çš„åµŒå…¥å‘é‡
            target_embedding, target_time = await self.get_embedding(target_text)
            total_target_time += target_time
            
            if target_embedding is None:
                semantic_score = 0.0
            else:
                semantic_score = cosine_similarity(interest_embedding, target_embedding)
            
            # ç›´æ¥ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦ä½œä¸ºæœ€ç»ˆåˆ†æ•°
            final_score = semantic_score
            
            results.append({
                "target_text": target_text,
                "final_score": final_score,
                "semantic_score": semantic_score,
                "timing": {
                    "target_embedding_time": target_time,
                    "item_total_time": target_time
                }
            })
        
        # æŒ‰åˆ†æ•°æ’åº
        results.sort(key=lambda x: x["final_score"], reverse=True)
        
        total_time = time.time() - total_start_time
        avg_target_time = total_target_time / len(target_texts) if target_texts else 0
        
        print(f"\n=== æ€§èƒ½ç»Ÿè®¡ ===")
        print(f"å…´è¶£æ–‡æœ¬åµŒå…¥è®¡ç®—è€—æ—¶: {interest_time:.3f}ç§’")
        print(f"ç›®æ ‡æ–‡æœ¬åµŒå…¥è®¡ç®—æ€»è€—æ—¶: {total_target_time:.3f}ç§’")
        print(f"ç›®æ ‡æ–‡æœ¬åµŒå…¥è®¡ç®—å¹³å‡è€—æ—¶: {avg_target_time:.3f}ç§’")
        print(f"æ€»è€—æ—¶: {total_time:.3f}ç§’")
        print(f"å¹³å‡æ¯ä¸ªç›®æ ‡æ–‡æœ¬å¤„ç†è€—æ—¶: {total_time / len(target_texts):.3f}ç§’")
        
        return results

    async def generate_paraphrases(self, original_text: str, num_sentences: int = 5) -> List[str]:
        """
        ä½¿ç”¨LLMç”Ÿæˆè¿‘ä¹‰å¥å­
        
        Args:
            original_text: åŸå§‹æ–‡æœ¬
            num_sentences: ç”Ÿæˆå¥å­æ•°é‡
            
        Returns:
            è¿‘ä¹‰å¥å­åˆ—è¡¨
        """
        try:
            # åˆ›å»ºLLMè¯·æ±‚å®ä¾‹
            llm_request = LLMRequest(
                model_set=model_config.model_task_config.replyer,
                request_type="paraphrase_generator"
            )
            
            # æ„å»ºç”Ÿæˆè¿‘ä¹‰å¥å­çš„æç¤ºè¯
            prompt = f"""è¯·ä¸ºä»¥ä¸‹å…´è¶£æè¿°ç”Ÿæˆ{num_sentences}ä¸ªæ„ä¹‰ç›¸è¿‘ä½†è¡¨è¾¾ä¸åŒçš„å¥å­ï¼š

åŸå§‹å…´è¶£æè¿°ï¼š{original_text}

è¦æ±‚ï¼š
1. ä¿æŒåŸæ„ä¸å˜ï¼Œä½†å°½é‡è‡ªç”±å‘æŒ¥ï¼Œä½¿ç”¨ä¸åŒçš„è¡¨è¾¾æ–¹å¼ï¼Œå†…å®¹ä¹Ÿå¯ä»¥æœ‰å·®å¼‚
2. å¥å­ç»“æ„è¦æœ‰æ‰€å˜åŒ–
3. å¯ä»¥é€‚å½“è°ƒæ•´è¯­æ°”å’Œé‡ç‚¹
4. æ¯ä¸ªå¥å­éƒ½è¦å®Œæ•´ä¸”è‡ªç„¶
5. åªè¿”å›å¥å­ï¼Œä¸è¦ç¼–å·ï¼Œæ¯è¡Œä¸€ä¸ªå¥å­

ç”Ÿæˆçš„è¿‘ä¹‰å¥å­ï¼š"""
            
            print(f"æ­£åœ¨ç”Ÿæˆè¿‘ä¹‰å¥å­...")
            content, (reasoning, model_name, tool_calls) = await llm_request.generate_response_async(prompt)
            
            # è§£æç”Ÿæˆçš„å¥å­
            sentences = []
            for line in content.strip().split('\n'):
                line = line.strip()
                if line and not line.startswith('ç”Ÿæˆ') and not line.startswith('è¿‘ä¹‰'):
                    sentences.append(line)
            
            # ç¡®ä¿è¿”å›æŒ‡å®šæ•°é‡çš„å¥å­
            sentences = sentences[:num_sentences]
            print(f"æˆåŠŸç”Ÿæˆ {len(sentences)} ä¸ªè¿‘ä¹‰å¥å­")
            print(f"ä½¿ç”¨çš„æ¨¡å‹: {model_name}")
            
            return sentences
            
        except Exception as e:
            print(f"ç”Ÿæˆè¿‘ä¹‰å¥å­å¤±è´¥: {e}")
            return []

    async def evaluate_all_paraphrases(self, original_text: str, target_texts: List[str], num_sentences: int = 5) -> Dict:
        """
        è¯„ä¼°åŸå§‹æ–‡æœ¬å’Œæ‰€æœ‰è¿‘ä¹‰å¥å­çš„å…´è¶£åº¦
        
        Args:
            original_text: åŸå§‹å…´è¶£æè¿°æ–‡æœ¬
            target_texts: ç›®æ ‡æ–‡æœ¬åˆ—è¡¨
            num_sentences: ç”Ÿæˆè¿‘ä¹‰å¥å­æ•°é‡
            
        Returns:
            åŒ…å«æ‰€æœ‰è¯„ä¼°ç»“æœçš„å­—å…¸
        """
        print(f"\n=== å¼€å§‹è¿‘ä¹‰å¥å­å…´è¶£åº¦è¯„ä¼° ===")
        print(f"åŸå§‹å…´è¶£æè¿°: {original_text}")
        print(f"ç›®æ ‡æ–‡æœ¬æ•°é‡: {len(target_texts)}")
        print(f"ç”Ÿæˆè¿‘ä¹‰å¥å­æ•°é‡: {num_sentences}")
        
        # ç”Ÿæˆè¿‘ä¹‰å¥å­
        paraphrases = await self.generate_paraphrases(original_text, num_sentences)
        if not paraphrases:
            print("ç”Ÿæˆè¿‘ä¹‰å¥å­å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ–‡æœ¬è¿›è¡Œè¯„ä¼°")
            paraphrases = []
        
        # æ‰€æœ‰å¾…è¯„ä¼°çš„æ–‡æœ¬ï¼ˆåŸå§‹æ–‡æœ¬ + è¿‘ä¹‰å¥å­ï¼‰
        all_texts = [original_text] + paraphrases
        
        # å¯¹æ¯ä¸ªæ–‡æœ¬è¿›è¡Œå…´è¶£åº¦è¯„ä¼°
        evaluation_results = {}
        
        for i, text in enumerate(all_texts):
            text_type = "åŸå§‹æ–‡æœ¬" if i == 0 else f"è¿‘ä¹‰å¥å­{i}"
            print(f"\n--- è¯„ä¼° {text_type} ---")
            print(f"æ–‡æœ¬å†…å®¹: {text}")
            
            # è®¡ç®—å…´è¶£åº¦
            results = await self.batch_calculate(text, target_texts)
            evaluation_results[text_type] = {
                "text": text,
                "results": results,
                "top_score": results[0]["final_score"] if results else 0.0,
                "average_score": sum(r["final_score"] for r in results) / len(results) if results else 0.0
            }
        
        return {
            "original_text": original_text,
            "paraphrases": paraphrases,
            "evaluations": evaluation_results,
            "summary": self._generate_summary(evaluation_results, target_texts)
        }
    
    def _generate_summary(self, evaluation_results: Dict, target_texts: List[str]) -> Dict:
        """ç”Ÿæˆè¯„ä¼°æ‘˜è¦ - å…³æ³¨ç›®æ ‡å¥å­çš„è¡¨ç°"""
        summary = {
            "best_performer": None,
            "worst_performer": None,
            "average_scores": {},
            "max_scores": {},
            "rankings": [],
            "target_stats": {},
            "target_rankings": []
        }
        
        scores = []
        
        for text_type, data in evaluation_results.items():
            scores.append({
                "text_type": text_type,
                "text": data["text"],
                "top_score": data["top_score"],
                "average_score": data["average_score"]
            })
        
        # æŒ‰top_scoreæ’åº
        scores.sort(key=lambda x: x["top_score"], reverse=True)
        
        summary["rankings"] = scores
        summary["best_performer"] = scores[0] if scores else None
        summary["worst_performer"] = scores[-1] if scores else None
        
        # è®¡ç®—åŸå§‹æ–‡æœ¬ç»Ÿè®¡
        original_score = next((s for s in scores if s["text_type"] == "åŸå§‹æ–‡æœ¬"), None)
        if original_score:
            summary["average_scores"]["original"] = original_score["average_score"]
            summary["max_scores"]["original"] = original_score["top_score"]
        
        # è®¡ç®—ç›®æ ‡å¥å­çš„ç»Ÿè®¡ä¿¡æ¯
        target_stats = {}
        for i, target_text in enumerate(target_texts):
            target_key = f"ç›®æ ‡{i+1}"
            scores_for_target = []
            
            # æ”¶é›†æ‰€æœ‰å…´è¶£æè¿°å¯¹è¯¥ç›®æ ‡æ–‡æœ¬çš„åˆ†æ•°
            for text_type, data in evaluation_results.items():
                for result in data["results"]:
                    if result["target_text"] == target_text:
                        scores_for_target.append(result["final_score"])
            
            if scores_for_target:
                target_stats[target_key] = {
                    "target_text": target_text,
                    "scores": scores_for_target,
                    "average": sum(scores_for_target) / len(scores_for_target),
                    "max": max(scores_for_target),
                    "min": min(scores_for_target),
                    "std": (sum((x - sum(scores_for_target) / len(scores_for_target)) ** 2 for x in scores_for_target) / len(scores_for_target)) ** 0.5
                }
        
        summary["target_stats"] = target_stats
        
        # æŒ‰å¹³å‡åˆ†å¯¹ç›®æ ‡æ–‡æœ¬æ’åº
        target_rankings = []
        for target_key, stats in target_stats.items():
            target_rankings.append({
                "target_key": target_key,
                "target_text": stats["target_text"],
                "average_score": stats["average"],
                "max_score": stats["max"],
                "min_score": stats["min"],
                "std_score": stats["std"]
            })
        
        target_rankings.sort(key=lambda x: x["average_score"], reverse=True)
        summary["target_rankings"] = target_rankings
        
        # è®¡ç®—ç›®æ ‡æ–‡æœ¬çš„æ•´ä½“ç»Ÿè®¡
        if target_rankings:
            all_target_averages = [t["average_score"] for t in target_rankings]
            all_target_scores = []
            for stats in target_stats.values():
                all_target_scores.extend(stats["scores"])
            
            summary["target_overall"] = {
                "avg_of_averages": sum(all_target_averages) / len(all_target_averages),
                "overall_max": max(all_target_scores),
                "overall_min": min(all_target_scores),
                "best_target": target_rankings[0]["target_text"],
                "worst_target": target_rankings[-1]["target_text"]
            }
        
        return summary


async def run_single_test():
    """è¿è¡Œå•ä¸ªæµ‹è¯•"""
    print("å•ä¸ªå…´è¶£åº¦æµ‹è¯•")
    print("=" * 40)
    
    # è¾“å…¥å…´è¶£æ–‡æœ¬
    # interest_text = input("è¯·è¾“å…¥å…´è¶£æè¿°æ–‡æœ¬: ").strip()
    # if not interest_text:
    #     print("å…´è¶£æè¿°ä¸èƒ½ä¸ºç©º")
    #     return
    
    interest_text ="å¯¹æŠ€æœ¯ç›¸å…³è¯é¢˜ï¼Œæ¸¸æˆå’ŒåŠ¨æ¼«ç›¸å…³è¯é¢˜æ„Ÿå…´è¶£ï¼Œä¹Ÿå¯¹æ—¥å¸¸è¯é¢˜æ„Ÿå…´è¶£ï¼Œä¸å–œæ¬¢å¤ªè¿‡æ²‰é‡ä¸¥è‚ƒçš„è¯é¢˜"
    
    # è¾“å…¥ç›®æ ‡æ–‡æœ¬
    print("è¯·è¾“å…¥ç›®æ ‡æ–‡æœ¬ (è¾“å…¥ç©ºè¡Œç»“æŸ):")
    import random
    target_texts = [
        "AveMujicaéå¸¸å¥½çœ‹ï¼Œä½ çœ‹äº†å—",
        "æ˜æ—¥æ–¹èˆŸè¿™ä¸ªæ¸¸æˆæŒºå¥½ç©çš„",
        "ä½ èƒ½ä¸èƒ½è¯´ç‚¹æ­£ç»çš„",
        "æ˜æ—¥æ–¹èˆŸæŒºå¥½ç©çš„",
        "ä½ çš„åå­—éå¸¸å¥½çœ‹ï¼Œä½ çœ‹äº†å—",
        "ã€Šä½ çš„åå­—ã€‹éå¸¸å¥½çœ‹ï¼Œä½ çœ‹äº†å—",
        "æˆ‘ä»¬æ¥èŠèŠè‹è”æ”¿æ²»å§",
        "è½»éŸ³å°‘å¥³éå¸¸å¥½çœ‹ï¼Œä½ çœ‹äº†å—",
        "æˆ‘è¿˜æŒºå–œæ¬¢æ‰“æ¸¸æˆçš„",
        "æˆ‘å˜ä¸ªåŸç¥ç©å®¶å•Š",
        "æˆ‘å¿ƒä¹°äº†PlayStation5",
        "ç›´æ¥Steam",
        "æœ‰æ²¡æœ‰R"
    ]
    random.shuffle(target_texts)
    # while True:
    #     line = input().strip()
    #     if not line:
    #         break
    #     target_texts.append(line)
    
    # if not target_texts:
    #     print("ç›®æ ‡æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
    #     return
    
    # è®¡ç®—å…´è¶£åº¦
    scorer = InterestScorer()
    results = await scorer.batch_calculate(interest_text, target_texts)
    
    # æ˜¾ç¤ºç»“æœ
    print(f"\nå…´è¶£åº¦æ’åºç»“æœ:")
    print("-" * 80)
    print(f"{'æ’å':<4} {'æœ€ç»ˆåˆ†æ•°':<10} {'è¯­ä¹‰åˆ†æ•°':<10} {'è€—æ—¶(ç§’)':<10} {'ç›®æ ‡æ–‡æœ¬'}")
    print("-" * 80)
    
    for j, result in enumerate(results):
        target_text = result['target_text']
        if len(target_text) > 40:
            target_text = target_text[:37] + "..."
        
        timing = result.get('timing', {})
        item_time = timing.get('item_total_time', 0.0)
        
        print(f"{j+1:<4} {result['final_score']:<10.3f} {result['semantic_score']:<10.3f} "
              f"{item_time:<10.3f} {target_text}")


async def run_paraphrase_test():
    """è¿è¡Œè¿‘ä¹‰å¥å­æµ‹è¯•"""
    print("è¿‘ä¹‰å¥å­å…´è¶£åº¦å¯¹æ¯”æµ‹è¯•")
    print("=" * 40)
    
    # è¾“å…¥å…´è¶£æ–‡æœ¬
    interest_text = "å¯¹æŠ€æœ¯ç›¸å…³è¯é¢˜ï¼Œæ¸¸æˆå’ŒåŠ¨æ¼«ç›¸å…³è¯é¢˜æ„Ÿå…´è¶£ï¼Œæ¯”å¦‚æ˜æ—¥æ–¹èˆŸå’ŒåŸç¥ï¼Œä¹Ÿå¯¹æ—¥å¸¸è¯é¢˜æ„Ÿå…´è¶£ï¼Œä¸å–œæ¬¢å¤ªè¿‡æ²‰é‡ä¸¥è‚ƒçš„è¯é¢˜"
    
    # è¾“å…¥ç›®æ ‡æ–‡æœ¬
    print("è¯·è¾“å…¥ç›®æ ‡æ–‡æœ¬ (è¾“å…¥ç©ºè¡Œç»“æŸ):")
    # target_texts = []
    # while True:
    #     line = input().strip()
    #     if not line:
    #         break
    #     target_texts.append(line)
    target_texts = [
    "AveMujicaéå¸¸å¥½çœ‹ï¼Œä½ çœ‹äº†å—",
    "æ˜æ—¥æ–¹èˆŸè¿™ä¸ªæ¸¸æˆæŒºå¥½ç©çš„",
    "ä½ èƒ½ä¸èƒ½è¯´ç‚¹æ­£ç»çš„",
    "æ˜æ—¥æ–¹èˆŸæŒºå¥½ç©çš„",
    "ä½ çš„åå­—éå¸¸å¥½çœ‹ï¼Œä½ çœ‹äº†å—",
    "ã€Šä½ çš„åå­—ã€‹éå¸¸å¥½çœ‹ï¼Œä½ çœ‹äº†å—",
    "æˆ‘ä»¬æ¥èŠèŠè‹è”æ”¿æ²»å§",
    "è½»éŸ³å°‘å¥³éå¸¸å¥½çœ‹ï¼Œä½ çœ‹äº†å—",
    "æˆ‘è¿˜æŒºå–œæ¬¢æ‰“æ¸¸æˆçš„",
    "åˆšåŠ å¥½å‹å°±è§†å¥¸ç©ºé—´14æ¡",
    "å¯ä¹è€å¤§åŠ æˆ‘å¥½å‹ï¼Œæˆ‘å…ˆæ—¥ä¸€éç©ºé—´",
    "é¸Ÿä¸€èŒ¬èŒ¬çš„",
    "å¯ä¹å¯ä»¥æ˜¯mï¼Œç¾¤å‹å¯ä»¥æ˜¯s"
    ]
    
    if not target_texts:
        print("ç›®æ ‡æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        return
    
    # åˆ›å»ºè¯„ä¼°å™¨
    scorer = InterestScorer()
    
    # è¿è¡Œè¯„ä¼°
    result = await scorer.evaluate_all_paraphrases(interest_text, target_texts, num_sentences=5)
    
    # æ˜¾ç¤ºç»“æœ
    display_paraphrase_results(result, target_texts)


def display_paraphrase_results(result: Dict, target_texts: List[str]):
    """æ˜¾ç¤ºè¿‘ä¹‰å¥å­è¯„ä¼°ç»“æœ"""
    print("\n" + "=" * 80)
    print("è¿‘ä¹‰å¥å­å…´è¶£åº¦è¯„ä¼°ç»“æœ")
    print("=" * 80)
    
    # æ˜¾ç¤ºç›®æ ‡æ–‡æœ¬
    print(f"\nğŸ“‹ ç›®æ ‡æ–‡æœ¬åˆ—è¡¨:")
    print("-" * 40)
    for i, target in enumerate(target_texts):
        print(f"{i+1}. {target}")
    
    # æ˜¾ç¤ºç”Ÿæˆçš„è¿‘ä¹‰å¥å­
    print(f"\nğŸ“ ç”Ÿæˆçš„è¿‘ä¹‰å¥å­ (ä½œä¸ºå…´è¶£æè¿°):")
    print("-" * 40)
    for i, paraphrase in enumerate(result["paraphrases"]):
        print(f"{i+1}. {paraphrase}")
    
    # æ˜¾ç¤ºæ‘˜è¦
    summary = result["summary"]
    print(f"\nğŸ“Š è¯„ä¼°æ‘˜è¦:")
    print("-" * 40)
    
    if summary["best_performer"]:
        print(f"æœ€ä½³è¡¨ç°: {summary['best_performer']['text_type']} (æœ€é«˜åˆ†: {summary['best_performer']['top_score']:.3f})")
    
    if summary["worst_performer"]:
        print(f"æœ€å·®è¡¨ç°: {summary['worst_performer']['text_type']} (æœ€é«˜åˆ†: {summary['worst_performer']['top_score']:.3f})")
    
    print(f"åŸå§‹æ–‡æœ¬å¹³å‡åˆ†: {summary['average_scores'].get('original', 0):.3f}")
    
    # æ˜¾ç¤ºç›®æ ‡æ–‡æœ¬çš„æ•´ä½“ç»Ÿè®¡
    if "target_overall" in summary:
        overall = summary["target_overall"]
        print(f"\nğŸ“ˆ ç›®æ ‡æ–‡æœ¬æ•´ä½“ç»Ÿè®¡:")
        print("-" * 40)
        print(f"ç›®æ ‡æ–‡æœ¬æ•°é‡: {len(summary['target_rankings'])}")
        print(f"å¹³å‡åˆ†çš„å¹³å‡å€¼: {overall['avg_of_averages']:.3f}")
        print(f"æ‰€æœ‰åŒ¹é…ä¸­çš„æœ€é«˜åˆ†: {overall['overall_max']:.3f}")
        print(f"æ‰€æœ‰åŒ¹é…ä¸­çš„æœ€ä½åˆ†: {overall['overall_min']:.3f}")
        print(f"æœ€ä½³åŒ¹é…ç›®æ ‡: {overall['best_target'][:50]}...")
        print(f"æœ€å·®åŒ¹é…ç›®æ ‡: {overall['worst_target'][:50]}...")
    
    # æ˜¾ç¤ºç›®æ ‡æ–‡æœ¬æ’å
    if "target_rankings" in summary and summary["target_rankings"]:
        print(f"\nğŸ† ç›®æ ‡æ–‡æœ¬æ’å (æŒ‰å¹³å‡åˆ†):")
        print("-" * 80)
        print(f"{'æ’å':<4} {'å¹³å‡åˆ†':<8} {'æœ€é«˜åˆ†':<8} {'æœ€ä½åˆ†':<8} {'æ ‡å‡†å·®':<8} {'ç›®æ ‡æ–‡æœ¬'}")
        print("-" * 80)
        
        for i, target in enumerate(summary["target_rankings"]):
            target_text = target["target_text"][:40] + "..." if len(target["target_text"]) > 40 else target["target_text"]
            print(f"{i+1:<4} {target['average_score']:<8.3f} {target['max_score']:<8.3f} {target['min_score']:<8.3f} {target['std_score']:<8.3f} {target_text}")
    
    # æ˜¾ç¤ºæ¯ä¸ªç›®æ ‡æ–‡æœ¬çš„è¯¦ç»†åˆ†æ•°åˆ†å¸ƒ
    if "target_stats" in summary:
        print(f"\nğŸ“Š ç›®æ ‡æ–‡æœ¬è¯¦ç»†åˆ†æ•°åˆ†å¸ƒ:")
        print("-" * 80)
        
        for target_key, stats in summary["target_stats"].items():
            print(f"\n{target_key}: {stats['target_text']}")
            print(f"  å¹³å‡åˆ†: {stats['average']:.3f}")
            print(f"  æœ€é«˜åˆ†: {stats['max']:.3f}")
            print(f"  æœ€ä½åˆ†: {stats['min']:.3f}")
            print(f"  æ ‡å‡†å·®: {stats['std']:.3f}")
            print(f"  æ‰€æœ‰åˆ†æ•°: {[f'{s:.3f}' for s in stats['scores']]}")
    
    # æ˜¾ç¤ºæœ€ä½³å’Œæœ€å·®å…´è¶£æè¿°çš„ç›®æ ‡è¡¨ç°å¯¹æ¯”
    if summary["best_performer"] and summary["worst_performer"]:
        print(f"\nğŸ” æœ€ä½³ vs æœ€å·®å…´è¶£æè¿°å¯¹æ¯”:")
        print("-" * 80)
        
        best_data = result["evaluations"][summary["best_performer"]["text_type"]]
        worst_data = result["evaluations"][summary["worst_performer"]["text_type"]]
        
        print(f"æœ€ä½³å…´è¶£æè¿°: {summary['best_performer']['text']}")
        print(f"æœ€å·®å…´è¶£æè¿°: {summary['worst_performer']['text']}")
        print(f"")
        print(f"{'ç›®æ ‡æ–‡æœ¬':<30} {'æœ€ä½³åˆ†æ•°':<10} {'æœ€å·®åˆ†æ•°':<10} {'å·®å€¼'}")
        print("-" * 60)
        
        for best_result, worst_result in zip(best_data["results"], worst_data["results"]):
            if best_result["target_text"] == worst_result["target_text"]:
                diff = best_result["final_score"] - worst_result["final_score"]
                target_text = best_result["target_text"][:27] + "..." if len(best_result["target_text"]) > 30 else best_result["target_text"]
                print(f"{target_text:<30} {best_result['final_score']:<10.3f} {worst_result['final_score']:<10.3f} {diff:+.3f}")
    
    # æ˜¾ç¤ºæ’å
    print(f"\nğŸ† å…´è¶£æè¿°æ€§èƒ½æ’å:")
    print("-" * 80)
    print(f"{'æ’å':<4} {'æ–‡æœ¬ç±»å‹':<10} {'æœ€é«˜åˆ†':<8} {'å¹³å‡åˆ†':<8} {'å…´è¶£æè¿°å†…å®¹'}")
    print("-" * 80)
    
    for i, item in enumerate(summary["rankings"]):
        text_content = item["text"][:40] + "..." if len(item["text"]) > 40 else item["text"]
        print(f"{i+1:<4} {item['text_type']:<10} {item['top_score']:<8.3f} {item['average_score']:<8.3f} {text_content}")
    
    # æ˜¾ç¤ºæ¯ä¸ªå…´è¶£æè¿°çš„è¯¦ç»†ç»“æœ
    print(f"\nğŸ” è¯¦ç»†ç»“æœ:")
    print("-" * 80)
    
    for text_type, data in result["evaluations"].items():
        print(f"\n--- {text_type} ---")
        print(f"å…´è¶£æè¿°: {data['text']}")
        print(f"æœ€é«˜åˆ†: {data['top_score']:.3f}")
        print(f"å¹³å‡åˆ†: {data['average_score']:.3f}")
        
        # æ˜¾ç¤ºå‰3ä¸ªåŒ¹é…ç»“æœ
        top_results = data["results"][:3]
        print(f"å‰3ä¸ªåŒ¹é…çš„ç›®æ ‡æ–‡æœ¬:")
        for j, result_item in enumerate(top_results):
            print(f"  {j+1}. åˆ†æ•°: {result_item['final_score']:.3f} - {result_item['target_text']}")
    
    # æ˜¾ç¤ºå¯¹æ¯”è¡¨æ ¼
    print(f"\nğŸ“ˆ å…´è¶£æè¿°å¯¹æ¯”è¡¨æ ¼:")
    print("-" * 100)
    header = f"{'å…´è¶£æè¿°':<20}"
    for i, target in enumerate(target_texts):
        target_name = f"ç›®æ ‡{i+1}"
        header += f" {target_name:<12}"
    print(header)
    print("-" * 100)
    
    # åŸå§‹æ–‡æœ¬è¡Œ
    original_line = f"{'åŸå§‹æ–‡æœ¬':<20}"
    original_data = result["evaluations"]["åŸå§‹æ–‡æœ¬"]["results"]
    for i in range(len(target_texts)):
        if i < len(original_data):
            original_line += f" {original_data[i]['final_score']:<12.3f}"
        else:
            original_line += f" {'-':<12}"
    print(original_line)
    
    # è¿‘ä¹‰å¥å­è¡Œ
    for i, paraphrase in enumerate(result["paraphrases"]):
        text_type = f"è¿‘ä¹‰å¥å­{i+1}"
        line = f"{text_type:<20}"
        paraphrase_data = result["evaluations"][text_type]["results"]
        for j in range(len(target_texts)):
            if j < len(paraphrase_data):
                line += f" {paraphrase_data[j]['final_score']:<12.3f}"
            else:
                line += f" {'-':<12}"
        print(line)


def main():
    """ä¸»å‡½æ•°"""
    print("åŸºäºEmbeddingçš„å…´è¶£åº¦è®¡ç®—æµ‹è¯•å·¥å…·")
    print("1. å•ä¸ªå…´è¶£åº¦æµ‹è¯•")
    print("2. è¿‘ä¹‰å¥å­å…´è¶£åº¦å¯¹æ¯”æµ‹è¯•")
    
    choice = input("\nè¯·é€‰æ‹© (1/2): ").strip()
    
    if choice == "1":
        asyncio.run(run_single_test())
    elif choice == "2":
        asyncio.run(run_paraphrase_test())
    else:
        print("æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    main()