# 只有同意了EULA和PRIVACY协议才可以部署麦麦
# 配置以下的选项为true表示你同意了EULA和PRIVACY条款
# https://github.com/MaiM-with-u/MaiBot/blob/main/EULA.md
# https://github.com/MaiM-with-u/MaiBot/blob/main/PRIVACY.md
EULA_AGREE: false
PRIVACY_AGREE: false

# 麦麦Adapter的部署配置
adapter:

  image:
    repository: unclas/maimbot-adapter
    tag: main-20250807151247
    pullPolicy: IfNotPresent
    pullSecrets: [ ]

  resources: { }

  nodeSelector: { }
  tolerations: [ ]

  # 配置adapter的napcat websocket service
  # adapter会启动一个websocket服务端，用于与napcat通信
  # 这里的选项可以帮助你自定义服务端口
  # ！！！默认不使用NodePort。如果通过NodePort将服务端口映射到公网可能会被恶意客户端连接，请自行使用中间件鉴权！！！
  service:
    type: ClusterIP  # ClusterIP / NodePort 指定NodePort可以将内网的websocket端口映射到物理节点的端口
    port: 8095  # websocket监听端口ClusterIP的端口
    nodePort:  # 仅在设置NodePort类型时有效，不指定则会随机分配端口

  persistence:
    storageClass:
    accessModes:
      - ReadWriteOnce
    size: 1Gi

# 麦麦本体的部署配置
core:

  image:
    repository: sengokucola/maibot
    tag: main-d919c34
    pullPolicy: IfNotPresent
    pullSecrets: [ ]

  resources: { }

  nodeSelector: { }
  tolerations: [ ]

  persistence:
    storageClass:
    accessModes:
      - ReadWriteOnce
    size: 10Gi

# 麦麦的运行统计看板配置
# 麦麦每隔一段时间会自动输出html格式的运行统计报告，此统计报告可以作为静态网页访问
# 此功能默认禁用。如果你认为报告可以被公开访问（报告包含联系人/群组名称、模型token花费信息等），则可以启用此功能
# 如果启用此功能，你也可以考虑使用中间件进行鉴权，保护隐私信息
statistics_dashboard:

  enabled: false  # 是否启用运行统计看板

  replicaCount: 1

  image:
    repository: nginx
    tag: latest
    pullPolicy: IfNotPresent
    pullSecrets: [ ]

  resources: { }

  nodeSelector: { }
  tolerations: [ ]

  service:
    type: ClusterIP  # ClusterIP / NodePort 指定NodePort可以将内网的服务端口映射到物理节点的端口
    port: 80  # 服务端口
    nodePort:  # 仅在设置NodePort类型时有效，不指定则会随机分配端口
  ingress:
    enabled: false
    className: nginx
    annotations: { }
    host: maim-statistics.example.com  # 访问运行统计看板的域名
    path: /
    pathType: Prefix

  persistence:
    storageClass:
    # 如果你希望运行统计看板服务与麦麦本体运行在不同的节点（多活部署），那么需要ReadWriteMany访问模式
    # 注意：ReadWriteMany特性需要存储类底层支持
    accessModes:
      - ReadWriteOnce
    size: 100Mi

# napcat的部署配置
# ！！！napcat部署完毕后，务必修改默认密码！！！
napcat:

  # 考虑到复用外部napcat实例的情况，napcat部署已被解耦
  # 如果你有外部部署的napcat，则可以修改下面的enabled为false，本次不会重复部署napcat
  # 如果没有外部部署的napcat，默认会捆绑部署napcat，不需要修改此项
  enabled: true

  image:
    repository: mlikiowa/napcat-docker
    tag: v4.8.98
    pullPolicy: IfNotPresent
    pullSecrets: [ ]

  resources: { }

  nodeSelector: { }
  tolerations: [ ]

  # napcat进程的权限，默认不是特权用户
  permission:
    uid: 1000
    gid: 1000

  # 配置napcat web面板的service
  service:
    type: ClusterIP  # ClusterIP / NodePort 指定NodePort可以将内网的服务端口映射到物理节点的端口
    port: 6099  # 服务端口
    nodePort:  # 仅在设置NodePort类型时有效，不指定则会随机分配端口

  # 配置napcat web面板的ingress
  ingress:
    enabled: false  # 是否启用
    className: nginx
    annotations: { }
    host: napcat.example.com  # 暴露napcat web面板使用的域名
    path: /
    pathType: Prefix

  persistence:
    storageClass:
    accessModes:
      - ReadWriteOnce
    size: 5Gi

# sqlite-web的部署配置
sqlite_web:

  # 通过sqlite-web可以在网页上操作麦麦的数据库，方便调试。不部署对麦麦的运行无影响
  # 默认不会捆绑部署sqlite-web，如果你需要部署，请修改下面的enabled为true
  # ！！！sqlite-web服务无鉴权，暴露在公网上十分危险，推荐使用集群ClusterIP内网访问！！！
  # ！！！如果一定要暴露在公网，请自行使用中间件鉴权！！！
  enabled: false

  image:
    repository: coleifer/sqlite-web
    tag: latest
    pullPolicy: IfNotPresent
    pullSecrets: [ ]

  resources: { }

  nodeSelector: { }
  tolerations: [ ]

  # 配置sqlite-web面板的service
  # ！！！默认不使用NodePort。如果使用NodePort暴露到公网，请自行使用中间件鉴权！！！
  service:
    type: ClusterIP  # ClusterIP / NodePort 指定NodePort可以将内网的服务端口映射到物理节点的端口
    port: 8080  # 服务端口
    nodePort:  # 仅在设置NodePort类型时有效，不指定则会随机分配端口

  # 配置sqlite-web面板的ingress
  # ！！！默认不使用ingress。如果使用ingress暴露到公网，请自行使用中间件鉴权！！！
  ingress:
    enabled: false  # 是否启用
    className: nginx
    annotations: { }
    host: maim-sqlite.example.com  # 暴露websocket使用的域名
    path: /
    pathType: Prefix

# 麦麦各部分组件的运行配置文件
config:

  # adapter的config.toml
  adapter_config: |
    [inner]
    version = "0.1.1" # 版本号
    # 请勿修改版本号，除非你知道自己在做什么
    
    [nickname] # 现在没用
    nickname = ""
    
    [napcat_server] # Napcat连接的ws服务设置
    heartbeat_interval = 30 # 与Napcat设置的心跳相同（按秒计）
    
    [chat] # 黑白名单功能
    group_list_type = "whitelist" # 群组名单类型，可选为：whitelist, blacklist
    group_list = []               # 群组名单
    # 当group_list_type为whitelist时，只有群组名单中的群组可以聊天
    # 当group_list_type为blacklist时，群组名单中的任何群组无法聊天
    private_list_type = "whitelist" # 私聊名单类型，可选为：whitelist, blacklist
    private_list = []               # 私聊名单
    # 当private_list_type为whitelist时，只有私聊名单中的用户可以聊天
    # 当private_list_type为blacklist时，私聊名单中的任何用户无法聊天
    ban_user_id = []   # 全局禁止名单（全局禁止名单中的用户无法进行任何聊天）
    ban_qq_bot = false # 是否屏蔽QQ官方机器人
    enable_poke = true # 是否启用戳一戳功能
    
    [voice] # 发送语音设置
    use_tts = false # 是否使用tts语音（请确保你配置了tts并有对应的adapter）
    
    [debug]
    level = "INFO" # 日志等级（DEBUG, INFO, WARNING, ERROR, CRITICAL）

  # core的model_config.toml
  core_model_config: |
    [inner]
    version = "1.3.0"
    
    # 配置文件版本号迭代规则同bot_config.toml
    
    [[api_providers]] # API服务提供商（可以配置多个）
    name = "DeepSeek"                       # API服务商名称（可随意命名，在models的api-provider中需使用这个命名）
    base_url = "https://api.deepseek.cn/v1" # API服务商的BaseURL
    api_key = "your-api-key-here"           # API密钥（请替换为实际的API密钥）
    client_type = "openai"                  # 请求客户端（可选，默认值为"openai"，使用gimini等Google系模型时请配置为"gemini"）
    max_retry = 2                           # 最大重试次数（单个模型API调用失败，最多重试的次数）
    timeout = 30                            # API请求超时时间（单位：秒）
    retry_interval = 10                     # 重试间隔时间（单位：秒）
    
    [[api_providers]] # SiliconFlow的API服务商配置
    name = "SiliconFlow"
    base_url = "https://api.siliconflow.cn/v1"
    api_key = "your-siliconflow-api-key"
    client_type = "openai"
    max_retry = 2
    timeout = 30
    retry_interval = 10
    
    [[api_providers]] # 特殊：Google的Gimini使用特殊API，与OpenAI格式不兼容，需要配置client为"gemini"
    name = "Google"
    base_url = "https://api.google.com/v1"
    api_key = "your-google-api-key-1"
    client_type = "gemini"
    max_retry = 2
    timeout = 30
    retry_interval = 10
    
    
    [[models]] # 模型（可以配置多个）
    model_identifier = "deepseek-chat" # 模型标识符（API服务商提供的模型标识符）
    name = "deepseek-v3"               # 模型名称（可随意命名，在后面中需使用这个命名）
    api_provider = "DeepSeek"          # API服务商名称（对应在api_providers中配置的服务商名称）
    price_in = 2.0                     # 输入价格（用于API调用统计，单位：元/ M token）（可选，若无该字段，默认值为0）
    price_out = 8.0                    # 输出价格（用于API调用统计，单位：元/ M token）（可选，若无该字段，默认值为0）
    #force_stream_mode = true          # 强制流式输出模式（若模型不支持非流式输出，请取消该注释，启用强制流式输出，若无该字段，默认值为false）
    
    [[models]]
    model_identifier = "Pro/deepseek-ai/DeepSeek-V3"
    name = "siliconflow-deepseek-v3"
    api_provider = "SiliconFlow"
    price_in = 2.0
    price_out = 8.0
    
    [[models]]
    model_identifier = "Pro/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B"
    name = "deepseek-r1-distill-qwen-32b"
    api_provider = "SiliconFlow"
    price_in = 4.0
    price_out = 16.0
    
    [[models]]
    model_identifier = "Qwen/Qwen3-8B"
    name = "qwen3-8b"
    api_provider = "SiliconFlow"
    price_in = 0
    price_out = 0
    [models.extra_params] # 可选的额外参数配置
    enable_thinking = false # 不启用思考
    
    [[models]]
    model_identifier = "Qwen/Qwen3-14B"
    name = "qwen3-14b"
    api_provider = "SiliconFlow"
    price_in = 0.5
    price_out = 2.0
    [models.extra_params] # 可选的额外参数配置
    enable_thinking = false # 不启用思考
    
    [[models]]
    model_identifier = "Qwen/Qwen3-30B-A3B"
    name = "qwen3-30b"
    api_provider = "SiliconFlow"
    price_in = 0.7
    price_out = 2.8
    [models.extra_params] # 可选的额外参数配置
    enable_thinking = false # 不启用思考
    
    [[models]]
    model_identifier = "Qwen/Qwen2.5-VL-72B-Instruct"
    name = "qwen2.5-vl-72b"
    api_provider = "SiliconFlow"
    price_in = 4.13
    price_out = 4.13
    
    [[models]]
    model_identifier = "FunAudioLLM/SenseVoiceSmall"
    name = "sensevoice-small"
    api_provider = "SiliconFlow"
    price_in = 0
    price_out = 0
    
    [[models]]
    model_identifier = "BAAI/bge-m3"
    name = "bge-m3"
    api_provider = "SiliconFlow"
    price_in = 0
    price_out = 0
    
    
    [model_task_config.utils] # 在麦麦的一些组件中使用的模型，例如表情包模块，取名模块，关系模块，是麦麦必须的模型
    model_list = ["siliconflow-deepseek-v3"] # 使用的模型列表，每个子项对应上面的模型名称(name)
    temperature = 0.2                        # 模型温度，新V3建议0.1-0.3
    max_tokens = 800                         # 最大输出token数
    
    [model_task_config.utils_small] # 在麦麦的一些组件中使用的小模型，消耗量较大，建议使用速度较快的小模型
    model_list = ["qwen3-8b"]
    temperature = 0.7
    max_tokens = 800
    
    [model_task_config.replyer] # 首要回复模型，还用于表达器和表达方式学习
    model_list = ["siliconflow-deepseek-v3"]
    temperature = 0.2                        # 模型温度，新V3建议0.1-0.3
    max_tokens = 800
    
    [model_task_config.planner] #决策：负责决定麦麦该做什么的模型
    model_list = ["siliconflow-deepseek-v3"]
    temperature = 0.3
    max_tokens = 800
    
    [model_task_config.emotion] #负责麦麦的情绪变化
    model_list = ["siliconflow-deepseek-v3"]
    temperature = 0.3
    max_tokens = 800
    
    [model_task_config.vlm] # 图像识别模型
    model_list = ["qwen2.5-vl-72b"]
    max_tokens = 800
    
    [model_task_config.voice] # 语音识别模型
    model_list = ["sensevoice-small"]
    
    [model_task_config.tool_use] #工具调用模型，需要使用支持工具调用的模型
    model_list = ["qwen3-14b"]
    temperature = 0.7
    max_tokens = 800
    
    #嵌入模型
    [model_task_config.embedding]
    model_list = ["bge-m3"]
    
    #------------LPMM知识库模型------------
    
    [model_task_config.lpmm_entity_extract] # 实体提取模型
    model_list = ["siliconflow-deepseek-v3"]
    temperature = 0.2
    max_tokens = 800
    
    [model_task_config.lpmm_rdf_build] # RDF构建模型
    model_list = ["siliconflow-deepseek-v3"]
    temperature = 0.2
    max_tokens = 800
    
    [model_task_config.lpmm_qa] # 问答模型
    model_list = ["deepseek-r1-distill-qwen-32b"]
    temperature = 0.7
    max_tokens = 800

  # core的bot_config.toml
  core_bot_config: |
    [inner]
    version = "6.4.6"
    
    #----以下是给开发人员阅读的，如果你只是部署了麦麦，不需要阅读----
    #如果你想要修改配置文件，请递增version的值
    #如果新增项目，请阅读src/config/official_configs.py中的说明
    #
    # 版本格式：主版本号.次版本号.修订号，版本号递增规则如下：
    #     主版本号：MMC版本更新
    #     次版本号：配置文件内容大更新
    #     修订号：配置文件内容小更新
    #----以上是给开发人员阅读的，如果你只是部署了麦麦，不需要阅读----
    
    [bot]
    platform = "qq"
    qq_account = 1145141919810 # 麦麦的QQ账号
    nickname = "麦麦" # 麦麦的昵称
    alias_names = ["麦叠", "牢麦"] # 麦麦的别名
    
    [personality]
    # 建议50字以内，描述人格的核心特质
    personality_core = "是一个女孩子"
    # 人格的细节，描述人格的一些侧面
    personality_side = "有时候说话不过脑子,喜欢开玩笑, 有时候会表现得无语,有时候会喜欢说一些奇怪的话"
    #アイデンティティがない 生まれないらららら
    # 可以描述外貌，性别，身高，职业，属性等等描述
    identity = "年龄为19岁,是女孩子,身高为160cm,有黑色的短发"
    
    # 描述麦麦说话的表达风格，表达习惯，如要修改，可以酌情新增内容
    reply_style = "回复可以简短一些。可以参考贴吧，知乎和微博的回复风格，回复不要浮夸，不要用夸张修辞，平淡一些。不要浮夸，不要夸张修辞。"
    
    compress_personality = false # 是否压缩人格，压缩后会精简人格信息，节省token消耗并提高回复性能，但是会丢失一些信息，如果人设不长，可以关闭
    compress_identity = true # 是否压缩身份，压缩后会精简身份信息，节省token消耗并提高回复性能，但是会丢失一些信息，如果不长，可以关闭
    
    [expression]
    # 表达学习配置
    learning_list = [ # 表达学习配置列表，支持按聊天流配置
        ["", "enable", "enable", "1.0"],  # 全局配置：使用表达，启用学习，学习强度1.0
        ["qq:1919810:group", "enable", "enable", "1.5"],  # 特定群聊配置：使用表达，启用学习，学习强度1.5
        ["qq:114514:private", "enable", "disable", "0.5"],  # 特定私聊配置：使用表达，禁用学习，学习强度0.5
        # 格式说明：
        # 第一位: chat_stream_id，空字符串表示全局配置
        # 第二位: 是否使用学到的表达 ("enable"/"disable")
        # 第三位: 是否学习表达 ("enable"/"disable")
        # 第四位: 学习强度（浮点数），影响学习频率，最短学习时间间隔 = 300/学习强度（秒）
        # 学习强度越高，学习越频繁；学习强度越低，学习越少
    ]
    
    expression_groups = [
        ["qq:1919810:private","qq:114514:private","qq:1111111:group"], # 在这里设置互通组，相同组的chat_id会共享学习到的表达方式
        # 格式：["qq:123456:private","qq:654321:group"]
        # 注意：如果为群聊，则需要设置为group，如果设置为私聊，则需要设置为private
    ]
    
    
    [chat] #麦麦的聊天设置
    talk_frequency = 0.5
    # 麦麦活跃度，越高，麦麦回复越多，范围0-1
    focus_value = 0.5
    # 麦麦的专注度，越高越容易持续连续对话，可能消耗更多token, 范围0-1
    
    max_context_size = 20 # 上下文长度
    
    mentioned_bot_inevitable_reply = true # 提及 bot 大概率回复
    at_bot_inevitable_reply = true # @bot 或 提及bot 大概率回复
    
    focus_value_adjust = [
        ["", "8:00,1", "12:00,0.8", "18:00,1", "01:00,0.3"],
        ["qq:114514:group", "12:20,0.6", "16:10,0.5", "20:10,0.8", "00:10,0.3"],
        ["qq:1919810:private", "8:20,0.5", "12:10,0.8", "20:10,1", "00:10,0.2"]
    ]
    
    talk_frequency_adjust = [
        ["", "8:00,0.5", "12:00,0.6", "18:00,0.8", "01:00,0.3"],
        ["qq:114514:group", "12:20,0.3", "16:10,0.5", "20:10,0.4", "00:10,0.1"],
        ["qq:1919810:private", "8:20,0.3", "12:10,0.4", "20:10,0.5", "00:10,0.1"]
    ]
    # 基于聊天流的个性化活跃度和专注度配置
    # 格式：[["platform:chat_id:type", "HH:MM,frequency", "HH:MM,frequency", ...], ...]
    
    # 全局配置示例：
    # [["", "8:00,1", "12:00,2", "18:00,1.5", "00:00,0.5"]]
    
    # 特定聊天流配置示例：
    # [
    #     ["", "8:00,1", "12:00,1.2", "18:00,1.5", "01:00,0.6"],  # 全局默认配置
    #     ["qq:1026294844:group", "12:20,1", "16:10,2", "20:10,1", "00:10,0.3"],  # 特定群聊配置
    #     ["qq:729957033:private", "8:20,1", "12:10,2", "20:10,1.5", "00:10,0.2"]  # 特定私聊配置
    # ]
    
    # 说明：
    # - 当第一个元素为空字符串""时，表示全局默认配置
    # - 当第一个元素为"platform:id:type"格式时，表示特定聊天流配置
    # - 后续元素是"时间,频率"格式，表示从该时间开始使用该活跃度，直到下一个时间点
    # - 优先级：特定聊天流配置 > 全局配置 > 默认 talk_frequency
    
    
    [relationship]
    enable_relationship = true # 是否启用关系系统
    relation_frequency = 1 # 关系频率，麦麦构建关系的频率
    
    [message_receive]
    # 以下是消息过滤，可以根据规则过滤特定消息，将不会读取这些消息
    ban_words = [
        # "403","张三"
        ]
    
    ban_msgs_regex = [
        # 需要过滤的消息（原始消息）匹配的正则表达式，匹配到的消息将被过滤，若不了解正则表达式请勿修改
        #"https?://[^\\s]+", # 匹配https链接
        #"\\d{4}-\\d{2}-\\d{2}", # 匹配日期
    ]
    
    [tool]
    enable_tool = false # 是否在普通聊天中启用工具
    
    [mood]
    enable_mood = true # 是否启用情绪系统
    mood_update_threshold = 1 # 情绪更新阈值,越高，更新越慢
    
    [emoji]
    emoji_chance = 0.6 # 麦麦激活表情包动作的概率
    
    max_reg_num = 60 # 表情包最大注册数量
    do_replace = true # 开启则在达到最大数量时删除（替换）表情包，关闭则达到最大数量时不会继续收集表情包
    check_interval = 10 # 检查表情包（注册，破损，删除）的时间间隔(分钟)
    steal_emoji = true # 是否偷取表情包，让麦麦可以将一些表情包据为己有
    content_filtration = false  # 是否启用表情包过滤，只有符合该要求的表情包才会被保存
    filtration_prompt = "符合公序良俗" # 表情包过滤要求，只有符合该要求的表情包才会被保存
    
    [memory]
    enable_memory = true # 是否启用记忆系统
    memory_build_frequency = 1 # 记忆构建频率 越高，麦麦学习越多
    memory_compress_rate = 0.1 # 记忆压缩率 控制记忆精简程度 建议保持默认,调高可以获得更多信息，但是冗余信息也会增多
    
    forget_memory_interval = 3000 # 记忆遗忘间隔 单位秒   间隔越低，麦麦遗忘越频繁，记忆更精简，但更难学习
    memory_forget_time = 48 #多长时间后的记忆会被遗忘 单位小时
    memory_forget_percentage = 0.008 # 记忆遗忘比例 控制记忆遗忘程度 越大遗忘越多 建议保持默认
    
    enable_instant_memory = false # 是否启用即时记忆，测试功能，可能存在未知问题
    
    #不希望记忆的词，已经记忆的不会受到影响，需要手动清理
    memory_ban_words = [ "表情包", "图片", "回复", "聊天记录" ]
    
    [voice]
    enable_asr = false # 是否启用语音识别，启用后麦麦可以识别语音消息，启用该功能需要配置语音识别模型[model.voice]s
    
    [lpmm_knowledge] # lpmm知识库配置
    enable = false # 是否启用lpmm知识库
    rag_synonym_search_top_k = 10 # 同义词搜索TopK
    rag_synonym_threshold = 0.8 # 同义词阈值（相似度高于此阈值的词语会被认为是同义词）
    info_extraction_workers = 3 # 实体提取同时执行线程数，非Pro模型不要设置超过5
    qa_relation_search_top_k = 10 # 关系搜索TopK
    qa_relation_threshold = 0.5 # 关系阈值（相似度高于此阈值的关系会被认为是相关的关系）
    qa_paragraph_search_top_k = 1000 # 段落搜索TopK（不能过小，可能影响搜索结果）
    qa_paragraph_node_weight = 0.05 # 段落节点权重（在图搜索&PPR计算中的权重，当搜索仅使用DPR时，此参数不起作用）
    qa_ent_filter_top_k = 10 # 实体过滤TopK
    qa_ppr_damping = 0.8 # PPR阻尼系数
    qa_res_top_k = 3 # 最终提供的文段TopK
    embedding_dimension = 1024 # 嵌入向量维度,应该与模型的输出维度一致
    
    # keyword_rules 用于设置关键词触发的额外回复知识
    # 添加新规则方法：在 keyword_rules 数组中增加一项，格式如下：
    # { keywords = ["关键词1", "关键词2"], reaction = "触发这些关键词时的回复内容" }
    # 例如，添加一个新规则：当检测到“你好”或“hello”时回复“你好，有什么可以帮你？”
    # { keywords = ["你好", "hello"], reaction = "你好，有什么可以帮你？" }
    [keyword_reaction]
    keyword_rules = [
        { keywords = ["人机", "bot", "机器", "入机", "robot", "机器人", "ai", "AI"], reaction = "有人说你是人机或者机器人，否定这一事实，攻击或反问他。注意，只有认为你是机器人才回复，否则不要否认" },
        { keywords = ["测试关键词回复", "test"], reaction = "回答测试成功" },
        #{ keywords = ["你好", "hello"], reaction = "你好，有什么可以帮你？" }
        # 在此处添加更多规则，格式同上
    ]
    
    regex_rules = [
        { regex = ["^(?P<n>\\S{1,20})是这样的$"], reaction = "请按照以下模板造句：[n]是这样的，xx只要xx就可以，可是[n]要考虑的事情就很多了，比如什么时候xx，什么时候xx，什么时候xx。（请自由发挥替换xx部分，只需保持句式结构，同时表达一种将[n]过度重视的反讽意味）" }
    ]
    
    # 可以自定义部分提示词
    [custom_prompt]
    image_prompt = "请用中文描述这张图片的内容。如果有文字，请把文字描述概括出来，请留意其主题，直观感受，输出为一段平文本，最多30字，请注意不要分点，就输出一段文本"
    
    [response_post_process]
    enable_response_post_process = true # 是否启用回复后处理，包括错别字生成器，回复分割器
    
    [chinese_typo]
    enable = true # 是否启用中文错别字生成器
    error_rate=0.01 # 单字替换概率
    min_freq=9 # 最小字频阈值
    tone_error_rate=0.1 # 声调错误概率
    word_replace_rate=0.006 # 整词替换概率
    
    [response_splitter]
    enable = true # 是否启用回复分割器
    max_length = 512 # 回复允许的最大长度
    max_sentence_num = 8 # 回复允许的最大句子数
    enable_kaomoji_protection = false # 是否启用颜文字保护
    
    [log]
    date_style = "m-d H:i:s" # 日期格式
    log_level_style = "lite" # 日志级别样式,可选FULL，compact，lite
    color_text = "full" # 日志文本颜色，可选none，title，full
    log_level = "INFO" # 全局日志级别（向下兼容，优先级低于下面的分别设置）
    console_log_level = "INFO" # 控制台日志级别，可选: DEBUG, INFO, WARNING, ERROR, CRITICAL
    file_log_level = "DEBUG" # 文件日志级别，可选: DEBUG, INFO, WARNING, ERROR, CRITICAL
    
    # 第三方库日志控制
    suppress_libraries = ["faiss","httpx", "urllib3", "asyncio", "websockets", "httpcore", "requests", "peewee", "openai","uvicorn","jieba"] # 完全屏蔽的库
    library_log_levels = { "aiohttp" = "WARNING"} # 设置特定库的日志级别
    
    [debug]
    show_prompt = false # 是否显示prompt
    
    [maim_message]
    auth_token = [] # 认证令牌，用于API验证，为空则不启用验证
    # 以下项目若要使用需要打开use_custom，并单独配置maim_message的服务器
    use_custom = false # 是否启用自定义的maim_message服务器，注意这需要设置新的端口，不能与.env重复
    host="127.0.0.1"
    port=8090
    mode="ws" # 支持ws和tcp两种模式
    use_wss = false # 是否使用WSS安全连接，只支持ws模式
    cert_file = "" # SSL证书文件路径，仅在use_wss=true时有效
    key_file = "" # SSL密钥文件路径，仅在use_wss=true时有效
    
    [telemetry] #发送统计信息，主要是看全球有多少只麦麦
    enable = true
    
    [experimental] #实验性功能
    enable_friend_chat = false # 是否启用好友聊天
