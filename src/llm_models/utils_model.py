import asyncio
import json
import re
from datetime import datetime
from typing import Tuple, Union, Dict, Any, Callable
import aiohttp
from aiohttp.client import ClientResponse
from src.common.logger import get_logger
import base64
from PIL import Image
import io
import os
import copy  # æ·»åŠ copyæ¨¡å—ç”¨äºæ·±æ‹·è´
from src.common.database.database import db  # ç¡®ä¿ db è¢«å¯¼å…¥ç”¨äº create_tables
from src.common.database.database_model import LLMUsage  # å¯¼å…¥ LLMUsage æ¨¡å‹
from src.config.config import global_config
from src.common.tcp_connector import get_tcp_connector
from rich.traceback import install

install(extra_lines=3)

logger = get_logger("model_utils")


class PayLoadTooLargeError(Exception):
    """è‡ªå®šä¹‰å¼‚å¸¸ç±»ï¼Œç”¨äºå¤„ç†è¯·æ±‚ä½“è¿‡å¤§é”™è¯¯"""

    def __init__(self, message: str):
        super().__init__(message)
        self.message = message

    def __str__(self):
        return "è¯·æ±‚ä½“è¿‡å¤§ï¼Œè¯·å°è¯•å‹ç¼©å›¾ç‰‡æˆ–å‡å°‘è¾“å…¥å†…å®¹ã€‚"


class RequestAbortException(Exception):
    """è‡ªå®šä¹‰å¼‚å¸¸ç±»ï¼Œç”¨äºå¤„ç†è¯·æ±‚ä¸­æ–­å¼‚å¸¸"""

    def __init__(self, message: str, response: ClientResponse):
        super().__init__(message)
        self.message = message
        self.response = response

    def __str__(self):
        return self.message


class PermissionDeniedException(Exception):
    """è‡ªå®šä¹‰å¼‚å¸¸ç±»ï¼Œç”¨äºå¤„ç†è®¿é—®æ‹’ç»çš„å¼‚å¸¸"""

    def __init__(self, message: str):
        super().__init__(message)
        self.message = message

    def __str__(self):
        return self.message


# å¸¸è§Error Code Mapping
error_code_mapping = {
    400: "å‚æ•°ä¸æ­£ç¡®",
    401: "API key é”™è¯¯ï¼Œè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥/config/bot_config.tomlå’Œ.envä¸­çš„é…ç½®æ˜¯å¦æ­£ç¡®å“¦~",
    402: "è´¦å·ä½™é¢ä¸è¶³",
    403: "éœ€è¦å®å,æˆ–ä½™é¢ä¸è¶³",
    404: "Not Found",
    429: "è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•",
    500: "æœåŠ¡å™¨å†…éƒ¨æ•…éšœ",
    503: "æœåŠ¡å™¨è´Ÿè½½è¿‡é«˜",
}


async def _safely_record(request_content: Dict[str, Any], payload: Dict[str, Any]):
    """å®‰å…¨åœ°è®°å½•è¯·æ±‚ä½“ï¼Œç”¨äºè°ƒè¯•æ—¥å¿—ï¼Œä¸ä¼šä¿®æ”¹åŸå§‹payloadå¯¹è±¡"""
    # åˆ›å»ºpayloadçš„æ·±æ‹·è´ï¼Œé¿å…ä¿®æ”¹åŸå§‹å¯¹è±¡
    safe_payload = copy.deepcopy(payload)
    
    image_base64: str = request_content.get("image_base64")
    image_format: str = request_content.get("image_format")
    if (
        image_base64
        and safe_payload
        and isinstance(safe_payload, dict)
        and "messages" in safe_payload
        and len(safe_payload["messages"]) > 0
    ):
        if isinstance(safe_payload["messages"][0], dict) and "content" in safe_payload["messages"][0]:
            content = safe_payload["messages"][0]["content"]
            if isinstance(content, list) and len(content) > 1 and "image_url" in content[1]:
                # åªä¿®æ”¹æ‹·è´çš„å¯¹è±¡ï¼Œç”¨äºå®‰å…¨çš„æ—¥å¿—è®°å½•
                safe_payload["messages"][0]["content"][1]["image_url"]["url"] = (
                    f"data:image/{image_format.lower() if image_format else 'jpeg'};base64,"
                    f"{image_base64[:10]}...{image_base64[-10:]}"
                )
    return safe_payload


class LLMRequest:
    # å®šä¹‰éœ€è¦è½¬æ¢çš„æ¨¡å‹åˆ—è¡¨ï¼Œä½œä¸ºç±»å˜é‡é¿å…é‡å¤
    MODELS_NEEDING_TRANSFORMATION = [
        "o1",
        "o1-2024-12-17",
        "o1-mini",
        "o1-mini-2024-09-12",
        "o1-preview",
        "o1-preview-2024-09-12",
        "o1-pro",
        "o1-pro-2025-03-19",
        "o3",
        "o3-2025-04-16",
        "o3-mini",
        "o3-mini-2025-01-31",
        "o4-mini",
        "o4-mini-2025-04-16",
    ]

    def __init__(self, model: dict, **kwargs):
        # å°†å¤§å†™çš„é…ç½®é”®è½¬æ¢ä¸ºå°å†™å¹¶ä»configä¸­è·å–å®é™…å€¼
        logger.debug(f"ğŸ” [æ¨¡å‹åˆå§‹åŒ–] å¼€å§‹åˆå§‹åŒ–æ¨¡å‹: {model.get('name', 'Unknown')}")
        logger.debug(f"ğŸ” [æ¨¡å‹åˆå§‹åŒ–] æ¨¡å‹é…ç½®: {model}")
        logger.debug(f"ğŸ” [æ¨¡å‹åˆå§‹åŒ–] é¢å¤–å‚æ•°: {kwargs}")
        
        try:
            # print(f"model['provider']: {model['provider']}")
            self.api_key = os.environ[f"{model['provider']}_KEY"]
            self.base_url = os.environ[f"{model['provider']}_BASE_URL"]
            logger.debug(f"ğŸ” [æ¨¡å‹åˆå§‹åŒ–] æˆåŠŸè·å–ç¯å¢ƒå˜é‡: {model['provider']}_KEY å’Œ {model['provider']}_BASE_URL")
        except AttributeError as e:
            logger.error(f"åŸå§‹ model dict ä¿¡æ¯ï¼š{model}")
            logger.error(f"é…ç½®é”™è¯¯ï¼šæ‰¾ä¸åˆ°å¯¹åº”çš„é…ç½®é¡¹ - {str(e)}")
            raise ValueError(f"é…ç½®é”™è¯¯ï¼šæ‰¾ä¸åˆ°å¯¹åº”çš„é…ç½®é¡¹ - {str(e)}") from e
        except KeyError:
            logger.warning(
                f"æ‰¾ä¸åˆ°{model['provider']}_KEYæˆ–{model['provider']}_BASE_URLç¯å¢ƒå˜é‡ï¼Œè¯·æ£€æŸ¥é…ç½®æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡è®¾ç½®ã€‚"
            )
        self.model_name: str = model["name"]
        self.params = kwargs

        # è®°å½•é…ç½®æ–‡ä»¶ä¸­å£°æ˜äº†å“ªäº›å‚æ•°ï¼ˆä¸ç®¡å€¼æ˜¯ä»€ä¹ˆï¼‰
        self.has_enable_thinking = "enable_thinking" in model
        self.has_thinking_budget = "thinking_budget" in model
        
        self.enable_thinking = model.get("enable_thinking", False)
        self.temp = model.get("temp", 0.7)
        self.thinking_budget = model.get("thinking_budget", 4096)
        self.stream = model.get("stream", False)
        self.pri_in = model.get("pri_in", 0)
        self.pri_out = model.get("pri_out", 0)
        self.max_tokens = model.get("max_tokens", global_config.model.model_max_output_length)
        # print(f"max_tokens: {self.max_tokens}")
        
        logger.debug("ğŸ” [æ¨¡å‹åˆå§‹åŒ–] æ¨¡å‹å‚æ•°è®¾ç½®å®Œæˆ:")
        logger.debug(f"   - model_name: {self.model_name}")
        logger.debug(f"   - has_enable_thinking: {self.has_enable_thinking}")
        logger.debug(f"   - enable_thinking: {self.enable_thinking}")
        logger.debug(f"   - has_thinking_budget: {self.has_thinking_budget}")
        logger.debug(f"   - thinking_budget: {self.thinking_budget}")
        logger.debug(f"   - temp: {self.temp}")
        logger.debug(f"   - stream: {self.stream}")
        logger.debug(f"   - max_tokens: {self.max_tokens}")
        logger.debug(f"   - base_url: {self.base_url}")

        # è·å–æ•°æ®åº“å®ä¾‹
        self._init_database()

        # ä» kwargs ä¸­æå– request_typeï¼Œå¦‚æœæ²¡æœ‰æä¾›åˆ™é»˜è®¤ä¸º "default"
        self.request_type = kwargs.pop("request_type", "default")
        logger.debug(f"ğŸ” [æ¨¡å‹åˆå§‹åŒ–] åˆå§‹åŒ–å®Œæˆï¼Œrequest_type: {self.request_type}")

    @staticmethod
    def _init_database():
        """åˆå§‹åŒ–æ•°æ®åº“é›†åˆ"""
        try:
            # ä½¿ç”¨ Peewee åˆ›å»ºè¡¨ï¼Œsafe=True è¡¨ç¤ºå¦‚æœè¡¨å·²å­˜åœ¨åˆ™ä¸ä¼šæŠ›å‡ºé”™è¯¯
            db.create_tables([LLMUsage], safe=True)
            # logger.debug("LLMUsage è¡¨å·²åˆå§‹åŒ–/ç¡®ä¿å­˜åœ¨ã€‚")
        except Exception as e:
            logger.error(f"åˆ›å»º LLMUsage è¡¨å¤±è´¥: {str(e)}")

    def _record_usage(
        self,
        prompt_tokens: int,
        completion_tokens: int,
        total_tokens: int,
        user_id: str = "system",
        request_type: str = None,
        endpoint: str = "/chat/completions",
    ):
        """è®°å½•æ¨¡å‹ä½¿ç”¨æƒ…å†µåˆ°æ•°æ®åº“
        Args:
            prompt_tokens: è¾“å…¥tokenæ•°
            completion_tokens: è¾“å‡ºtokenæ•°
            total_tokens: æ€»tokenæ•°
            user_id: ç”¨æˆ·IDï¼Œé»˜è®¤ä¸ºsystem
            request_type: è¯·æ±‚ç±»å‹
            endpoint: APIç«¯ç‚¹
        """
        # å¦‚æœ request_type ä¸º Noneï¼Œåˆ™ä½¿ç”¨å®ä¾‹å˜é‡ä¸­çš„å€¼
        if request_type is None:
            request_type = self.request_type

        try:
            # ä½¿ç”¨ Peewee æ¨¡å‹åˆ›å»ºè®°å½•
            LLMUsage.create(
                model_name=self.model_name,
                user_id=user_id,
                request_type=request_type,
                endpoint=endpoint,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
                cost=self._calculate_cost(prompt_tokens, completion_tokens),
                status="success",
                timestamp=datetime.now(),  # Peewee ä¼šå¤„ç† DateTimeField
            )
            logger.debug(
                f"Tokenä½¿ç”¨æƒ…å†µ - æ¨¡å‹: {self.model_name}, "
                f"ç”¨æˆ·: {user_id}, ç±»å‹: {request_type}, "
                f"æç¤ºè¯: {prompt_tokens}, å®Œæˆ: {completion_tokens}, "
                f"æ€»è®¡: {total_tokens}"
            )
        except Exception as e:
            logger.error(f"è®°å½•tokenä½¿ç”¨æƒ…å†µå¤±è´¥: {str(e)}")

    def _calculate_cost(self, prompt_tokens: int, completion_tokens: int) -> float:
        """è®¡ç®—APIè°ƒç”¨æˆæœ¬
        ä½¿ç”¨æ¨¡å‹çš„pri_inå’Œpri_outä»·æ ¼è®¡ç®—è¾“å…¥å’Œè¾“å‡ºçš„æˆæœ¬

        Args:
            prompt_tokens: è¾“å…¥tokenæ•°é‡
            completion_tokens: è¾“å‡ºtokenæ•°é‡

        Returns:
            float: æ€»æˆæœ¬ï¼ˆå…ƒï¼‰
        """
        # ä½¿ç”¨æ¨¡å‹çš„pri_inå’Œpri_outè®¡ç®—æˆæœ¬
        input_cost = (prompt_tokens / 1000000) * self.pri_in
        output_cost = (completion_tokens / 1000000) * self.pri_out
        return round(input_cost + output_cost, 6)

    async def _prepare_request(
        self,
        endpoint: str,
        prompt: str = None,
        image_base64: str = None,
        image_format: str = None,
        file_bytes: bytes = None,
        file_format: str = None,
        payload: dict = None,
        retry_policy: dict = None,
    ) -> Dict[str, Any]:
        """é…ç½®è¯·æ±‚å‚æ•°
        Args:
            endpoint: APIç«¯ç‚¹è·¯å¾„ (å¦‚ "chat/completions")
            prompt: promptæ–‡æœ¬
            image_base64: å›¾ç‰‡çš„base64ç¼–ç 
            image_format: å›¾ç‰‡æ ¼å¼
            file_bytes: æ–‡ä»¶çš„äºŒè¿›åˆ¶æ•°æ®
            file_format: æ–‡ä»¶æ ¼å¼
            payload: è¯·æ±‚ä½“æ•°æ®
            retry_policy: è‡ªå®šä¹‰é‡è¯•ç­–ç•¥
            request_type: è¯·æ±‚ç±»å‹
        """

        # åˆå¹¶é‡è¯•ç­–ç•¥
        default_retry = {
            "max_retries": 3,
            "base_wait": 10,
            "retry_codes": [429, 413, 500, 503],
            "abort_codes": [400, 401, 402, 403],
        }
        policy = {**default_retry, **(retry_policy or {})}

        api_url = f"{self.base_url.rstrip('/')}/{endpoint.lstrip('/')}"

        stream_mode = self.stream

        # æ„å»ºè¯·æ±‚ä½“
        if image_base64:
            payload = await self._build_payload(prompt, image_base64, image_format)
        elif file_bytes:
            payload = await self._build_formdata_payload(file_bytes, file_format)
        elif payload is None:
            payload = await self._build_payload(prompt)

        if not file_bytes:
            if stream_mode:
                payload["stream"] = stream_mode

            if self.temp != 0.7:
                payload["temperature"] = self.temp

            # æ·»åŠ enable_thinkingå‚æ•°ï¼ˆåªæœ‰é…ç½®æ–‡ä»¶ä¸­å£°æ˜äº†æ‰æ·»åŠ ï¼Œä¸ç®¡å€¼æ˜¯trueè¿˜æ˜¯falseï¼‰
            if self.has_enable_thinking:
                payload["enable_thinking"] = self.enable_thinking

            # æ·»åŠ thinking_budgetå‚æ•°ï¼ˆåªæœ‰é…ç½®æ–‡ä»¶ä¸­å£°æ˜äº†æ‰æ·»åŠ ï¼‰
            if self.has_thinking_budget:
                payload["thinking_budget"] = self.thinking_budget

            if self.max_tokens:
                payload["max_tokens"] = self.max_tokens

            # if "max_tokens" not in payload and "max_completion_tokens" not in payload:
            # payload["max_tokens"] = global_config.model.model_max_output_length
            # å¦‚æœ payload ä¸­ä¾ç„¶å­˜åœ¨ max_tokens ä¸”éœ€è¦è½¬æ¢ï¼Œåœ¨è¿™é‡Œè¿›è¡Œå†æ¬¡æ£€æŸ¥
            if self.model_name.lower() in self.MODELS_NEEDING_TRANSFORMATION and "max_tokens" in payload:
                payload["max_completion_tokens"] = payload.pop("max_tokens")

        return {
            "policy": policy,
            "payload": payload,
            "api_url": api_url,
            "stream_mode": stream_mode,
            "image_base64": image_base64,  # ä¿ç•™å¿…è¦çš„exceptionå¤„ç†æ‰€éœ€çš„åŸå§‹æ•°æ®
            "image_format": image_format,
            "file_bytes": file_bytes,
            "file_format": file_format,
            "prompt": prompt,
        }

    async def _execute_request(
        self,
        endpoint: str,
        prompt: str = None,
        image_base64: str = None,
        image_format: str = None,
        file_bytes: bytes = None,
        file_format: str = None,
        payload: dict = None,
        retry_policy: dict = None,
        response_handler: Callable = None,
        user_id: str = "system",
        request_type: str = None,
    ):
        """ç»Ÿä¸€è¯·æ±‚æ‰§è¡Œå…¥å£
        Args:
            endpoint: APIç«¯ç‚¹è·¯å¾„ (å¦‚ "chat/completions")
            prompt: promptæ–‡æœ¬
            image_base64: å›¾ç‰‡çš„base64ç¼–ç 
            image_format: å›¾ç‰‡æ ¼å¼
            file_bytes: æ–‡ä»¶çš„äºŒè¿›åˆ¶æ•°æ®
            file_format: æ–‡ä»¶æ ¼å¼
            payload: è¯·æ±‚ä½“æ•°æ®
            retry_policy: è‡ªå®šä¹‰é‡è¯•ç­–ç•¥
            response_handler: è‡ªå®šä¹‰å“åº”å¤„ç†å™¨
            user_id: ç”¨æˆ·ID
            request_type: è¯·æ±‚ç±»å‹
        """
        # è·å–è¯·æ±‚é…ç½®
        request_content = await self._prepare_request(
            endpoint, prompt, image_base64, image_format, file_bytes, file_format, payload, retry_policy
        )
        if request_type is None:
            request_type = self.request_type
        for retry in range(request_content["policy"]["max_retries"]):
            try:
                # ä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨å¤„ç†ä¼šè¯
                if file_bytes:
                    headers = await self._build_headers(is_formdata=True)
                else:
                    headers = await self._build_headers(is_formdata=False)
                # ä¼¼ä¹æ˜¯openaiæµå¼å¿…é¡»è¦çš„ä¸œè¥¿,ä¸è¿‡é˜¿é‡Œäº‘çš„qwq-plusåŠ äº†è¿™ä¸ªæ²¡æœ‰å½±å“
                if request_content["stream_mode"]:
                    headers["Accept"] = "text/event-stream"
                
                # æ·»åŠ è¯·æ±‚å‘é€å‰çš„è°ƒè¯•ä¿¡æ¯
                logger.debug(f"ğŸ” [è¯·æ±‚è°ƒè¯•] æ¨¡å‹ {self.model_name} å‡†å¤‡å‘é€è¯·æ±‚")
                logger.debug(f"ğŸ” [è¯·æ±‚è°ƒè¯•] API URL: {request_content['api_url']}")
                logger.debug(f"ğŸ” [è¯·æ±‚è°ƒè¯•] è¯·æ±‚å¤´: {await self._build_headers(no_key=True, is_formdata=file_bytes is not None)}")
                
                if not file_bytes:
                    # å®‰å…¨åœ°è®°å½•è¯·æ±‚ä½“ï¼ˆéšè—æ•æ„Ÿä¿¡æ¯ï¼‰
                    safe_payload = await _safely_record(request_content, request_content["payload"])
                    logger.debug(f"ğŸ” [è¯·æ±‚è°ƒè¯•] è¯·æ±‚ä½“: {json.dumps(safe_payload, indent=2, ensure_ascii=False)}")
                else:
                    logger.debug(f"ğŸ” [è¯·æ±‚è°ƒè¯•] æ–‡ä»¶ä¸Šä¼ è¯·æ±‚ï¼Œæ–‡ä»¶æ ¼å¼: {request_content['file_format']}")
                
                async with aiohttp.ClientSession(connector=await get_tcp_connector()) as session:
                    post_kwargs = {"headers": headers}
                    # form-dataæ•°æ®ä¸Šä¼ æ–¹å¼ä¸åŒ
                    if file_bytes:
                        post_kwargs["data"] = request_content["payload"]
                    else:
                        post_kwargs["json"] = request_content["payload"]

                    async with session.post(request_content["api_url"], **post_kwargs) as response:
                        handled_result = await self._handle_response(
                            response, request_content, retry, response_handler, user_id, request_type, endpoint
                        )
                        return handled_result

            except Exception as e:
                handled_payload, count_delta = await self._handle_exception(e, retry, request_content)
                retry += count_delta  # é™çº§ä¸è®¡å…¥é‡è¯•æ¬¡æ•°
                if handled_payload:
                    # å¦‚æœé™çº§æˆåŠŸï¼Œé‡æ–°æ„å»ºè¯·æ±‚ä½“
                    request_content["payload"] = handled_payload
                continue

        logger.error(f"æ¨¡å‹ {self.model_name} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼Œè¯·æ±‚ä»ç„¶å¤±è´¥")
        raise RuntimeError(f"æ¨¡å‹ {self.model_name} è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°ï¼ŒAPIè¯·æ±‚ä»ç„¶å¤±è´¥")

    async def _handle_response(
        self,
        response: ClientResponse,
        request_content: Dict[str, Any],
        retry_count: int,
        response_handler: Callable,
        user_id,
        request_type,
        endpoint,
    ):
        policy = request_content["policy"]
        stream_mode = request_content["stream_mode"]
        if response.status in policy["retry_codes"] or response.status in policy["abort_codes"]:
            await self._handle_error_response(response, retry_count, policy)
            return None

        response.raise_for_status()
        result = {}
        if stream_mode:
            # å°†æµå¼è¾“å‡ºè½¬åŒ–ä¸ºéæµå¼è¾“å‡º
            result = await self._handle_stream_output(response)
        else:
            result = await response.json()
        return (
            response_handler(result)
            if response_handler
            else self._default_response_handler(result, user_id, request_type, endpoint)
        )

    async def _handle_stream_output(self, response: ClientResponse) -> Dict[str, Any]:
        flag_delta_content_finished = False
        accumulated_content = ""
        usage = None  # åˆå§‹åŒ–usageå˜é‡ï¼Œé¿å…æœªå®šä¹‰é”™è¯¯
        reasoning_content = ""
        content = ""
        tool_calls = None  # åˆå§‹åŒ–å·¥å…·è°ƒç”¨å˜é‡

        async for line_bytes in response.content:
            try:
                line = line_bytes.decode("utf-8").strip()
                if not line:
                    continue
                if line.startswith("data:"):
                    data_str = line[5:].strip()
                    if data_str == "[DONE]":
                        break
                    try:
                        chunk = json.loads(data_str)
                        if flag_delta_content_finished:
                            chunk_usage = chunk.get("usage", None)
                            if chunk_usage:
                                usage = chunk_usage  # è·å–tokenç”¨é‡
                        else:
                            delta = chunk["choices"][0]["delta"]
                            delta_content = delta.get("content")
                            if delta_content is None:
                                delta_content = ""
                            accumulated_content += delta_content

                            # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
                            if "tool_calls" in delta:
                                if tool_calls is None:
                                    tool_calls = delta["tool_calls"]
                                else:
                                    # åˆå¹¶å·¥å…·è°ƒç”¨ä¿¡æ¯
                                    tool_calls.extend(delta["tool_calls"])

                            # æ£€æµ‹æµå¼è¾“å‡ºæ–‡æœ¬æ˜¯å¦ç»“æŸ
                            finish_reason = chunk["choices"][0].get("finish_reason")
                            if delta.get("reasoning_content", None):
                                reasoning_content += delta["reasoning_content"]
                            if finish_reason == "stop" or finish_reason == "tool_calls":
                                chunk_usage = chunk.get("usage", None)
                                if chunk_usage:
                                    usage = chunk_usage
                                    break
                                # éƒ¨åˆ†å¹³å°åœ¨æ–‡æœ¬è¾“å‡ºç»“æŸå‰ä¸ä¼šè¿”å›tokenç”¨é‡ï¼Œæ­¤æ—¶éœ€è¦å†è·å–ä¸€æ¬¡chunk
                                flag_delta_content_finished = True
                    except Exception as e:
                        logger.exception(f"æ¨¡å‹ {self.model_name} è§£ææµå¼è¾“å‡ºé”™è¯¯: {str(e)}")
            except Exception as e:
                if isinstance(e, GeneratorExit):
                    log_content = f"æ¨¡å‹ {self.model_name} æµå¼è¾“å‡ºè¢«ä¸­æ–­ï¼Œæ­£åœ¨æ¸…ç†èµ„æº..."
                else:
                    log_content = f"æ¨¡å‹ {self.model_name} å¤„ç†æµå¼è¾“å‡ºæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}"
                logger.warning(log_content)
                # ç¡®ä¿èµ„æºè¢«æ­£ç¡®æ¸…ç†
                try:
                    await response.release()
                except Exception as cleanup_error:
                    logger.error(f"æ¸…ç†èµ„æºæ—¶å‘ç”Ÿé”™è¯¯: {cleanup_error}")
                # è¿”å›å·²ç»ç´¯ç§¯çš„å†…å®¹
                content = accumulated_content
        if not content:
            content = accumulated_content
        think_match = re.search(r"<think>(.*?)</think>", content, re.DOTALL)
        if think_match:
            reasoning_content = think_match.group(1).strip()
        content = re.sub(r"<think>.*?</think>", "", content, flags=re.DOTALL).strip()

        # æ„å»ºæ¶ˆæ¯å¯¹è±¡
        message = {
            "content": content,
            "reasoning_content": reasoning_content,
        }

        # å¦‚æœæœ‰å·¥å…·è°ƒç”¨ï¼Œæ·»åŠ åˆ°æ¶ˆæ¯ä¸­
        if tool_calls:
            message["tool_calls"] = tool_calls

        result = {
            "choices": [{"message": message}],
            "usage": usage,
        }
        return result

    async def _handle_error_response(self, response: ClientResponse, retry_count: int, policy: Dict[str, Any]):
        if response.status in policy["retry_codes"]:
            wait_time = policy["base_wait"] * (2**retry_count)
            logger.warning(f"æ¨¡å‹ {self.model_name} é”™è¯¯ç : {response.status}, ç­‰å¾… {wait_time}ç§’åé‡è¯•")
            if response.status == 413:
                logger.warning("è¯·æ±‚ä½“è¿‡å¤§ï¼Œå°è¯•å‹ç¼©...")
                raise PayLoadTooLargeError("è¯·æ±‚ä½“è¿‡å¤§")
            elif response.status in [500, 503]:
                logger.error(
                    f"æ¨¡å‹ {self.model_name} é”™è¯¯ç : {response.status} - {error_code_mapping.get(response.status)}"
                )
                raise RuntimeError("æœåŠ¡å™¨è´Ÿè½½è¿‡é«˜ï¼Œæ¨¡å‹å›å¤å¤±è´¥QAQ")
            else:
                logger.warning(f"æ¨¡å‹ {self.model_name} è¯·æ±‚é™åˆ¶(429)ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•...")
                raise RuntimeError("è¯·æ±‚é™åˆ¶(429)")
        elif response.status in policy["abort_codes"]:
            # ç‰¹åˆ«å¤„ç†400é”™è¯¯ï¼Œæ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯
            if response.status == 400:
                logger.error(f"ğŸ” [è°ƒè¯•ä¿¡æ¯] æ¨¡å‹ {self.model_name} å‚æ•°é”™è¯¯ (400) - å¼€å§‹è¯¦ç»†è¯Šæ–­")
                logger.error(f"ğŸ” [è°ƒè¯•ä¿¡æ¯] æ¨¡å‹åç§°: {self.model_name}")
                logger.error(f"ğŸ” [è°ƒè¯•ä¿¡æ¯] APIåœ°å€: {self.base_url}")
                logger.error("ğŸ” [è°ƒè¯•ä¿¡æ¯] æ¨¡å‹é…ç½®å‚æ•°:")
                logger.error(f"   - enable_thinking: {self.enable_thinking}")
                logger.error(f"   - temp: {self.temp}")
                logger.error(f"   - thinking_budget: {self.thinking_budget}")
                logger.error(f"   - stream: {self.stream}")
                logger.error(f"   - max_tokens: {self.max_tokens}")
                logger.error(f"   - pri_in: {self.pri_in}")
                logger.error(f"   - pri_out: {self.pri_out}")
                logger.error(f"ğŸ” [è°ƒè¯•ä¿¡æ¯] åŸå§‹params: {self.params}")
                
                # å°è¯•è·å–æœåŠ¡å™¨è¿”å›çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯
                try:
                    error_text = await response.text()
                    logger.error(f"ğŸ” [è°ƒè¯•ä¿¡æ¯] æœåŠ¡å™¨è¿”å›çš„åŸå§‹é”™è¯¯å†…å®¹: {error_text}")
                    
                    try:
                        error_json = json.loads(error_text)
                        logger.error(f"ğŸ” [è°ƒè¯•ä¿¡æ¯] è§£æåçš„é”™è¯¯JSON: {json.dumps(error_json, indent=2, ensure_ascii=False)}")
                    except json.JSONDecodeError:
                        logger.error("ğŸ” [è°ƒè¯•ä¿¡æ¯] é”™è¯¯å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼")
                except Exception as e:
                    logger.error(f"ğŸ” [è°ƒè¯•ä¿¡æ¯] æ— æ³•è¯»å–é”™è¯¯å“åº”å†…å®¹: {str(e)}")
                
                raise RequestAbortException("å‚æ•°é”™è¯¯ï¼Œè¯·æ£€æŸ¥è°ƒè¯•ä¿¡æ¯", response)
            elif response.status != 403:
                raise RequestAbortException("è¯·æ±‚å‡ºç°é”™è¯¯ï¼Œä¸­æ–­å¤„ç†", response)
            else:
                raise PermissionDeniedException("æ¨¡å‹ç¦æ­¢è®¿é—®")

    async def _handle_exception(
        self, exception, retry_count: int, request_content: Dict[str, Any]
    ) -> Union[Tuple[Dict[str, Any], int], Tuple[None, int]]:
        policy = request_content["policy"]
        payload = request_content["payload"]
        wait_time = policy["base_wait"] * (2**retry_count)
        keep_request = False
        if retry_count < policy["max_retries"] - 1:
            keep_request = True
        if isinstance(exception, RequestAbortException):
            response = exception.response
            logger.error(
                f"æ¨¡å‹ {self.model_name} é”™è¯¯ç : {response.status} - {error_code_mapping.get(response.status)}"
            )
            
            # å¦‚æœæ˜¯400é”™è¯¯ï¼Œé¢å¤–è¾“å‡ºè¯·æ±‚ä½“ä¿¡æ¯ç”¨äºè°ƒè¯•
            if response.status == 400:
                logger.error("ğŸ” [å¼‚å¸¸è°ƒè¯•] 400é”™è¯¯ - è¯·æ±‚ä½“è°ƒè¯•ä¿¡æ¯:")
                try:
                    safe_payload = await _safely_record(request_content, payload)
                    logger.error(f"ğŸ” [å¼‚å¸¸è°ƒè¯•] å‘é€çš„è¯·æ±‚ä½“: {json.dumps(safe_payload, indent=2, ensure_ascii=False)}")
                except Exception as debug_error:
                    logger.error(f"ğŸ” [å¼‚å¸¸è°ƒè¯•] æ— æ³•å®‰å…¨è®°å½•è¯·æ±‚ä½“: {str(debug_error)}")
                    logger.error(f"ğŸ” [å¼‚å¸¸è°ƒè¯•] åŸå§‹payloadç±»å‹: {type(payload)}")
                    if isinstance(payload, dict):
                        logger.error(f"ğŸ” [å¼‚å¸¸è°ƒè¯•] åŸå§‹payloadé”®: {list(payload.keys())}")
            
            # print(request_content)
            # print(response)
            # å°è¯•è·å–å¹¶è®°å½•æœåŠ¡å™¨è¿”å›çš„è¯¦ç»†é”™è¯¯ä¿¡æ¯
            try:
                error_json = await response.json()
                if error_json and isinstance(error_json, list) and len(error_json) > 0:
                    # å¤„ç†å¤šä¸ªé”™è¯¯çš„æƒ…å†µ
                    for error_item in error_json:
                        if "error" in error_item and isinstance(error_item["error"], dict):
                            error_obj: dict = error_item["error"]
                            error_code = error_obj.get("code")
                            error_message = error_obj.get("message")
                            error_status = error_obj.get("status")
                            logger.error(
                                f"æœåŠ¡å™¨é”™è¯¯è¯¦æƒ…: ä»£ç ={error_code}, çŠ¶æ€={error_status}, æ¶ˆæ¯={error_message}"
                            )
                elif isinstance(error_json, dict) and "error" in error_json:
                    # å¤„ç†å•ä¸ªé”™è¯¯å¯¹è±¡çš„æƒ…å†µ
                    error_obj = error_json.get("error", {})
                    error_code = error_obj.get("code")
                    error_message = error_obj.get("message")
                    error_status = error_obj.get("status")
                    logger.error(f"æœåŠ¡å™¨é”™è¯¯è¯¦æƒ…: ä»£ç ={error_code}, çŠ¶æ€={error_status}, æ¶ˆæ¯={error_message}")
                else:
                    # è®°å½•åŸå§‹é”™è¯¯å“åº”å†…å®¹
                    logger.error(f"æœåŠ¡å™¨é”™è¯¯å“åº”: {error_json}")
            except Exception as e:
                logger.warning(f"æ— æ³•è§£ææœåŠ¡å™¨é”™è¯¯å“åº”: {str(e)}")
            raise RuntimeError(f"è¯·æ±‚è¢«æ‹’ç»: {error_code_mapping.get(response.status)}")

        elif isinstance(exception, PermissionDeniedException):
            # åªé’ˆå¯¹ç¡…åŸºæµåŠ¨çš„V3å’ŒR1è¿›è¡Œé™çº§å¤„ç†
            if self.model_name.startswith("Pro/deepseek-ai") and self.base_url == "https://api.siliconflow.cn/v1/":
                old_model_name = self.model_name
                self.model_name = self.model_name[4:]  # ç§»é™¤"Pro/"å‰ç¼€
                logger.warning(f"æ£€æµ‹åˆ°403é”™è¯¯ï¼Œæ¨¡å‹ä» {old_model_name} é™çº§ä¸º {self.model_name}")

                # å¯¹å…¨å±€é…ç½®è¿›è¡Œæ›´æ–°
                if global_config.model.replyer_2.get("name") == old_model_name:
                    global_config.model.replyer_2["name"] = self.model_name
                    logger.warning(f"å°†å…¨å±€é…ç½®ä¸­çš„ llm_normal æ¨¡å‹ä¸´æ—¶é™çº§è‡³{self.model_name}")
                if global_config.model.replyer_1.get("name") == old_model_name:
                    global_config.model.replyer_1["name"] = self.model_name
                    logger.warning(f"å°†å…¨å±€é…ç½®ä¸­çš„ llm_reasoning æ¨¡å‹ä¸´æ—¶é™çº§è‡³{self.model_name}")

                if payload and "model" in payload:
                    payload["model"] = self.model_name

                await asyncio.sleep(wait_time)
                return payload, -1
            raise RuntimeError(f"è¯·æ±‚è¢«æ‹’ç»: {error_code_mapping.get(403)}")

        elif isinstance(exception, PayLoadTooLargeError):
            if keep_request:
                image_base64 = request_content["image_base64"]
                compressed_image_base64 = compress_base64_image_by_scale(image_base64)
                new_payload = await self._build_payload(
                    request_content["prompt"], compressed_image_base64, request_content["image_format"]
                )
                return new_payload, 0
            else:
                return None, 0

        elif isinstance(exception, aiohttp.ClientError) or isinstance(exception, asyncio.TimeoutError):
            if keep_request:
                logger.error(f"æ¨¡å‹ {self.model_name} ç½‘ç»œé”™è¯¯ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•... é”™è¯¯: {str(exception)}")
                await asyncio.sleep(wait_time)
                return None, 0
            else:
                logger.critical(f"æ¨¡å‹ {self.model_name} ç½‘ç»œé”™è¯¯è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: {str(exception)}")
                raise RuntimeError(f"ç½‘ç»œè¯·æ±‚å¤±è´¥: {str(exception)}")

        elif isinstance(exception, aiohttp.ClientResponseError):
            # å¤„ç†aiohttpæŠ›å‡ºçš„ï¼Œé™¤äº†policyä¸­çš„statusçš„å“åº”é”™è¯¯
            if keep_request:
                logger.error(
                    f"æ¨¡å‹ {self.model_name} HTTPå“åº”é”™è¯¯ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•... çŠ¶æ€ç : {exception.status}, é”™è¯¯: {exception.message}"
                )
                try:
                    error_text = await exception.response.text()
                    error_json = json.loads(error_text)
                    if isinstance(error_json, list) and len(error_json) > 0:
                        # å¤„ç†å¤šä¸ªé”™è¯¯çš„æƒ…å†µ
                        for error_item in error_json:
                            if "error" in error_item and isinstance(error_item["error"], dict):
                                error_obj = error_item["error"]
                                logger.error(
                                    f"æ¨¡å‹ {self.model_name} æœåŠ¡å™¨é”™è¯¯è¯¦æƒ…: ä»£ç ={error_obj.get('code')}, "
                                    f"çŠ¶æ€={error_obj.get('status')}, "
                                    f"æ¶ˆæ¯={error_obj.get('message')}"
                                )
                    elif isinstance(error_json, dict) and "error" in error_json:
                        error_obj = error_json.get("error", {})
                        logger.error(
                            f"æ¨¡å‹ {self.model_name} æœåŠ¡å™¨é”™è¯¯è¯¦æƒ…: ä»£ç ={error_obj.get('code')}, "
                            f"çŠ¶æ€={error_obj.get('status')}, "
                            f"æ¶ˆæ¯={error_obj.get('message')}"
                        )
                    else:
                        logger.error(f"æ¨¡å‹ {self.model_name} æœåŠ¡å™¨é”™è¯¯å“åº”: {error_json}")
                except (json.JSONDecodeError, TypeError) as json_err:
                    logger.warning(
                        f"æ¨¡å‹ {self.model_name} å“åº”ä¸æ˜¯æœ‰æ•ˆçš„JSON: {str(json_err)}, åŸå§‹å†…å®¹: {error_text[:200]}"
                    )
                except Exception as parse_err:
                    logger.warning(f"æ¨¡å‹ {self.model_name} æ— æ³•è§£æå“åº”é”™è¯¯å†…å®¹: {str(parse_err)}")

                await asyncio.sleep(wait_time)
                return None, 0
            else:
                logger.critical(
                    f"æ¨¡å‹ {self.model_name} HTTPå“åº”é”™è¯¯è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°: çŠ¶æ€ç : {exception.status}, é”™è¯¯: {exception.message}"
                )
                # å®‰å…¨åœ°æ£€æŸ¥å’Œè®°å½•è¯·æ±‚è¯¦æƒ…
                handled_payload = await _safely_record(request_content, payload)
                logger.critical(
                    f"è¯·æ±‚å¤´: {await self._build_headers(no_key=True)} è¯·æ±‚ä½“: {str(handled_payload)[:100]}"
                )
                raise RuntimeError(
                    f"æ¨¡å‹ {self.model_name} APIè¯·æ±‚å¤±è´¥: çŠ¶æ€ç  {exception.status}, {exception.message}"
                )

        else:
            if keep_request:
                logger.error(f"æ¨¡å‹ {self.model_name} è¯·æ±‚å¤±è´¥ï¼Œç­‰å¾…{wait_time}ç§’åé‡è¯•... é”™è¯¯: {str(exception)}")
                await asyncio.sleep(wait_time)
                return None, 0
            else:
                logger.critical(f"æ¨¡å‹ {self.model_name} è¯·æ±‚å¤±è´¥: {str(exception)}")
                # å®‰å…¨åœ°æ£€æŸ¥å’Œè®°å½•è¯·æ±‚è¯¦æƒ…
                handled_payload = await _safely_record(request_content, payload)
                logger.critical(
                    f"è¯·æ±‚å¤´: {await self._build_headers(no_key=True)} è¯·æ±‚ä½“: {str(handled_payload)[:100]}"
                )
                raise RuntimeError(f"æ¨¡å‹ {self.model_name} APIè¯·æ±‚å¤±è´¥: {str(exception)}")

    async def _transform_parameters(self, params: dict) -> dict:
        """
        æ ¹æ®æ¨¡å‹åç§°è½¬æ¢å‚æ•°ï¼š
        - å¯¹äºéœ€è¦è½¬æ¢çš„OpenAI CoTç³»åˆ—æ¨¡å‹ï¼ˆä¾‹å¦‚ "o3-mini"ï¼‰ï¼Œåˆ é™¤ 'temperature' å‚æ•°ï¼Œ
        å¹¶å°† 'max_tokens' é‡å‘½åä¸º 'max_completion_tokens'
        """
        # å¤åˆ¶ä¸€ä»½å‚æ•°ï¼Œé¿å…ç›´æ¥ä¿®æ”¹åŸå§‹æ•°æ®
        new_params = dict(params)
        
        logger.debug(f"ğŸ” [å‚æ•°è½¬æ¢] æ¨¡å‹ {self.model_name} å¼€å§‹å‚æ•°è½¬æ¢")
        logger.debug(f"ğŸ” [å‚æ•°è½¬æ¢] æ˜¯å¦ä¸ºCoTæ¨¡å‹: {self.model_name.lower() in self.MODELS_NEEDING_TRANSFORMATION}")
        logger.debug(f"ğŸ” [å‚æ•°è½¬æ¢] CoTæ¨¡å‹åˆ—è¡¨: {self.MODELS_NEEDING_TRANSFORMATION}")

        if self.model_name.lower() in self.MODELS_NEEDING_TRANSFORMATION:
            logger.debug("ğŸ” [å‚æ•°è½¬æ¢] æ£€æµ‹åˆ°CoTæ¨¡å‹ï¼Œå¼€å§‹å‚æ•°è½¬æ¢")
            # åˆ é™¤ 'temperature' å‚æ•°ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œä½†é¿å…åˆ é™¤æˆ‘ä»¬åœ¨_build_payloadä¸­æ·»åŠ çš„è‡ªå®šä¹‰æ¸©åº¦
            if "temperature" in new_params and new_params["temperature"] == 0.7:
                removed_temp = new_params.pop("temperature")
                logger.debug(f"ğŸ” [å‚æ•°è½¬æ¢] ç§»é™¤é»˜è®¤temperatureå‚æ•°: {removed_temp}")
            # å¦‚æœå­˜åœ¨ 'max_tokens'ï¼Œåˆ™é‡å‘½åä¸º 'max_completion_tokens'
            if "max_tokens" in new_params:
                old_value = new_params["max_tokens"]
                new_params["max_completion_tokens"] = new_params.pop("max_tokens")
                logger.debug(f"ğŸ” [å‚æ•°è½¬æ¢] å‚æ•°é‡å‘½å: max_tokens({old_value}) -> max_completion_tokens({new_params['max_completion_tokens']})")
        else:
            logger.debug("ğŸ” [å‚æ•°è½¬æ¢] éCoTæ¨¡å‹ï¼Œæ— éœ€å‚æ•°è½¬æ¢")
            
        logger.debug(f"ğŸ” [å‚æ•°è½¬æ¢] è½¬æ¢å‰å‚æ•°: {params}")
        logger.debug(f"ğŸ” [å‚æ•°è½¬æ¢] è½¬æ¢åå‚æ•°: {new_params}")
        return new_params

    async def _build_formdata_payload(self, file_bytes: bytes, file_format: str) -> aiohttp.FormData:
        """æ„å»ºform-dataè¯·æ±‚ä½“"""
        # ç›®å‰åªé€‚é…äº†éŸ³é¢‘æ–‡ä»¶
        # å¦‚æœåç»­è¦æ”¯æŒå…¶ä»–ç±»å‹çš„æ–‡ä»¶ï¼Œå¯ä»¥åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šçš„å¤„ç†é€»è¾‘
        data = aiohttp.FormData()
        content_type_list = {
            "wav": "audio/wav",
            "mp3": "audio/mpeg",
            "ogg": "audio/ogg",
            "flac": "audio/flac",
            "aac": "audio/aac",
        }

        content_type = content_type_list.get(file_format)
        if not content_type:
            logger.warning(f"æš‚ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_format}")

        data.add_field(
            "file",
            io.BytesIO(file_bytes),
            filename=f"file.{file_format}",
            content_type=f"{content_type}",  # æ ¹æ®å®é™…æ–‡ä»¶ç±»å‹è®¾ç½®
        )
        data.add_field("model", self.model_name)
        return data

    async def _build_payload(self, prompt: str, image_base64: str = None, image_format: str = None) -> dict:
        """æ„å»ºè¯·æ±‚ä½“"""
        # å¤åˆ¶ä¸€ä»½å‚æ•°ï¼Œé¿å…ç›´æ¥ä¿®æ”¹ self.params
        logger.debug(f"ğŸ” [å‚æ•°æ„å»º] æ¨¡å‹ {self.model_name} å¼€å§‹æ„å»ºè¯·æ±‚ä½“")
        logger.debug(f"ğŸ” [å‚æ•°æ„å»º] åŸå§‹self.params: {self.params}")
        
        params_copy = await self._transform_parameters(self.params)
        logger.debug(f"ğŸ” [å‚æ•°æ„å»º] è½¬æ¢åçš„params_copy: {params_copy}")
        
        if image_base64:
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "text", "text": prompt},
                        {
                            "type": "image_url",
                            "image_url": {"url": f"data:image/{image_format.lower()};base64,{image_base64}"},
                        },
                    ],
                }
            ]
        else:
            messages = [{"role": "user", "content": prompt}]

        payload = {
            "model": self.model_name,
            "messages": messages,
            **params_copy,
        }
        
        logger.debug(f"ğŸ” [å‚æ•°æ„å»º] åŸºç¡€payloadæ„å»ºå®Œæˆ: {list(payload.keys())}")

        # æ·»åŠ tempå‚æ•°ï¼ˆå¦‚æœä¸æ˜¯é»˜è®¤å€¼0.7ï¼‰
        if self.temp != 0.7:
            payload["temperature"] = self.temp
            logger.debug(f"ğŸ” [å‚æ•°æ„å»º] æ·»åŠ temperatureå‚æ•°: {self.temp}")

        # æ·»åŠ enable_thinkingå‚æ•°ï¼ˆåªæœ‰é…ç½®æ–‡ä»¶ä¸­å£°æ˜äº†æ‰æ·»åŠ ï¼Œä¸ç®¡å€¼æ˜¯trueè¿˜æ˜¯falseï¼‰
        if self.has_enable_thinking:
            payload["enable_thinking"] = self.enable_thinking
            logger.debug(f"ğŸ” [å‚æ•°æ„å»º] æ·»åŠ enable_thinkingå‚æ•°: {self.enable_thinking}")

        # æ·»åŠ thinking_budgetå‚æ•°ï¼ˆåªæœ‰é…ç½®æ–‡ä»¶ä¸­å£°æ˜äº†æ‰æ·»åŠ ï¼‰
        if self.has_thinking_budget:
            payload["thinking_budget"] = self.thinking_budget
            logger.debug(f"ğŸ” [å‚æ•°æ„å»º] æ·»åŠ thinking_budgetå‚æ•°: {self.thinking_budget}")

        if self.max_tokens:
            payload["max_tokens"] = self.max_tokens
            logger.debug(f"ğŸ” [å‚æ•°æ„å»º] æ·»åŠ max_tokenså‚æ•°: {self.max_tokens}")

        # if "max_tokens" not in payload and "max_completion_tokens" not in payload:
        # payload["max_tokens"] = global_config.model.model_max_output_length
        # å¦‚æœ payload ä¸­ä¾ç„¶å­˜åœ¨ max_tokens ä¸”éœ€è¦è½¬æ¢ï¼Œåœ¨è¿™é‡Œè¿›è¡Œå†æ¬¡æ£€æŸ¥
        if self.model_name.lower() in self.MODELS_NEEDING_TRANSFORMATION and "max_tokens" in payload:
            old_value = payload["max_tokens"]
            payload["max_completion_tokens"] = payload.pop("max_tokens")
            logger.debug(f"ğŸ” [å‚æ•°æ„å»º] CoTæ¨¡å‹å‚æ•°è½¬æ¢: max_tokens({old_value}) -> max_completion_tokens({payload['max_completion_tokens']})")
        
        logger.debug(f"ğŸ” [å‚æ•°æ„å»º] æœ€ç»ˆpayloadé”®åˆ—è¡¨: {list(payload.keys())}")
        return payload

    def _default_response_handler(
        self, result: dict, user_id: str = "system", request_type: str = None, endpoint: str = "/chat/completions"
    ) -> Tuple:
        """é»˜è®¤å“åº”è§£æ"""
        if "choices" in result and result["choices"]:
            message = result["choices"][0]["message"]
            content = message.get("content", "")
            content, reasoning = self._extract_reasoning(content)
            reasoning_content = message.get("model_extra", {}).get("reasoning_content", "")
            if not reasoning_content:
                reasoning_content = message.get("reasoning_content", "")
                if not reasoning_content:
                    reasoning_content = reasoning

            # æå–å·¥å…·è°ƒç”¨ä¿¡æ¯
            tool_calls = message.get("tool_calls", None)

            # è®°å½•tokenä½¿ç”¨æƒ…å†µ
            usage = result.get("usage", {})
            if usage:
                prompt_tokens = usage.get("prompt_tokens", 0)
                completion_tokens = usage.get("completion_tokens", 0)
                total_tokens = usage.get("total_tokens", 0)
                self._record_usage(
                    prompt_tokens=prompt_tokens,
                    completion_tokens=completion_tokens,
                    total_tokens=total_tokens,
                    user_id=user_id,
                    request_type=request_type if request_type is not None else self.request_type,
                    endpoint=endpoint,
                )

            # åªæœ‰å½“tool_callså­˜åœ¨ä¸”ä¸ä¸ºç©ºæ—¶æ‰è¿”å›
            if tool_calls:
                logger.debug(f"æ£€æµ‹åˆ°å·¥å…·è°ƒç”¨: {tool_calls}")
                return content, reasoning_content, tool_calls
            else:
                return content, reasoning_content
        elif "text" in result and result["text"]:
            return result["text"]
        return "æ²¡æœ‰è¿”å›ç»“æœ", ""

    @staticmethod
    def _extract_reasoning(content: str) -> Tuple[str, str]:
        """CoTæ€ç»´é“¾æå–"""
        match = re.search(r"(?:<think>)?(.*?)</think>", content, re.DOTALL)
        content = re.sub(r"(?:<think>)?.*?</think>", "", content, flags=re.DOTALL, count=1).strip()
        if match:
            reasoning = match.group(1).strip()
        else:
            reasoning = ""
        return content, reasoning

    async def _build_headers(self, no_key: bool = False, is_formdata: bool = False) -> dict:
        """æ„å»ºè¯·æ±‚å¤´"""
        if no_key:
            if is_formdata:
                return {"Authorization": "Bearer **********"}
            return {"Authorization": "Bearer **********", "Content-Type": "application/json"}
        else:
            if is_formdata:
                return {"Authorization": f"Bearer {self.api_key}"}
            return {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
            # é˜²æ­¢å°æœ‹å‹ä»¬æˆªå›¾è‡ªå·±çš„key

    async def generate_response_for_image(self, prompt: str, image_base64: str, image_format: str) -> Tuple:
        """æ ¹æ®è¾“å…¥çš„æç¤ºå’Œå›¾ç‰‡ç”Ÿæˆæ¨¡å‹çš„å¼‚æ­¥å“åº”"""

        response = await self._execute_request(
            endpoint="/chat/completions", prompt=prompt, image_base64=image_base64, image_format=image_format
        )
        # æ ¹æ®è¿”å›å€¼çš„é•¿åº¦å†³å®šæ€ä¹ˆå¤„ç†
        if len(response) == 3:
            content, reasoning_content, tool_calls = response
            return content, reasoning_content, tool_calls
        else:
            content, reasoning_content = response
            return content, reasoning_content

    async def generate_response_for_voice(self, voice_bytes: bytes) -> Tuple:
        """æ ¹æ®è¾“å…¥çš„è¯­éŸ³æ–‡ä»¶ç”Ÿæˆæ¨¡å‹çš„å¼‚æ­¥å“åº”"""
        response = await self._execute_request(
            endpoint="/audio/transcriptions", file_bytes=voice_bytes, file_format="wav"
        )
        return response

    async def generate_response_async(self, prompt: str, **kwargs) -> Union[str, Tuple]:
        """å¼‚æ­¥æ–¹å¼æ ¹æ®è¾“å…¥çš„æç¤ºç”Ÿæˆæ¨¡å‹çš„å“åº”"""
        # æ„å»ºè¯·æ±‚ä½“ï¼Œä¸ç¡¬ç¼–ç max_tokens
        data = {
            "model": self.model_name,
            "messages": [{"role": "user", "content": prompt}],
            **self.params,
            **kwargs,
        }

        response = await self._execute_request(endpoint="/chat/completions", payload=data, prompt=prompt)
        # åŸæ ·è¿”å›å“åº”ï¼Œä¸åšå¤„ç†

        if len(response) == 3:
            content, reasoning_content, tool_calls = response
            return content, (reasoning_content, self.model_name, tool_calls)
        else:
            content, reasoning_content = response
            return content, (reasoning_content, self.model_name)

    async def get_embedding(self, text: str) -> Union[list, None]:
        """å¼‚æ­¥æ–¹æ³•ï¼šè·å–æ–‡æœ¬çš„embeddingå‘é‡

        Args:
            text: éœ€è¦è·å–embeddingçš„æ–‡æœ¬

        Returns:
            list: embeddingå‘é‡ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """

        if len(text) < 1:
            logger.debug("è¯¥æ¶ˆæ¯æ²¡æœ‰é•¿åº¦ï¼Œä¸å†å‘é€è·å–embeddingå‘é‡çš„è¯·æ±‚")
            return None

        def embedding_handler(result):
            """å¤„ç†å“åº”"""
            if "data" in result and len(result["data"]) > 0:
                # æå– token ä½¿ç”¨ä¿¡æ¯
                usage = result.get("usage", {})
                if usage:
                    prompt_tokens = usage.get("prompt_tokens", 0)
                    completion_tokens = usage.get("completion_tokens", 0)
                    total_tokens = usage.get("total_tokens", 0)
                    # è®°å½• token ä½¿ç”¨æƒ…å†µ
                    self._record_usage(
                        prompt_tokens=prompt_tokens,
                        completion_tokens=completion_tokens,
                        total_tokens=total_tokens,
                        user_id="system",  # å¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹ user_id
                        # request_type="embedding",  # è¯·æ±‚ç±»å‹ä¸º embedding
                        request_type=self.request_type,  # è¯·æ±‚ç±»å‹ä¸º text
                        endpoint="/embeddings",  # API ç«¯ç‚¹
                    )
                    return result["data"][0].get("embedding", None)
                return result["data"][0].get("embedding", None)
            return None

        embedding = await self._execute_request(
            endpoint="/embeddings",
            prompt=text,
            payload={"model": self.model_name, "input": text, "encoding_format": "float"},
            retry_policy={"max_retries": 2, "base_wait": 6},
            response_handler=embedding_handler,
        )
        return embedding


def compress_base64_image_by_scale(base64_data: str, target_size: int = 0.8 * 1024 * 1024) -> str:
    """å‹ç¼©base64æ ¼å¼çš„å›¾ç‰‡åˆ°æŒ‡å®šå¤§å°
    Args:
        base64_data: base64ç¼–ç çš„å›¾ç‰‡æ•°æ®
        target_size: ç›®æ ‡æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œé»˜è®¤0.8MB
    Returns:
        str: å‹ç¼©åçš„base64å›¾ç‰‡æ•°æ®
    """
    try:
        # å°†base64è½¬æ¢ä¸ºå­—èŠ‚æ•°æ®
        # ç¡®ä¿base64å­—ç¬¦ä¸²åªåŒ…å«ASCIIå­—ç¬¦
        if isinstance(base64_data, str):
            base64_data = base64_data.encode("ascii", errors="ignore").decode("ascii")
        image_data = base64.b64decode(base64_data)

        # å¦‚æœå·²ç»å°äºç›®æ ‡å¤§å°ï¼Œç›´æ¥è¿”å›åŸå›¾
        if len(image_data) <= 2 * 1024 * 1024:
            return base64_data

        # å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºå›¾ç‰‡å¯¹è±¡
        img = Image.open(io.BytesIO(image_data))

        # è·å–åŸå§‹å°ºå¯¸
        original_width, original_height = img.size

        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        scale = min(1.0, (target_size / len(image_data)) ** 0.5)

        # è®¡ç®—æ–°çš„å°ºå¯¸
        new_width = int(original_width * scale)
        new_height = int(original_height * scale)

        # åˆ›å»ºå†…å­˜ç¼“å†²åŒº
        output_buffer = io.BytesIO()

        # å¦‚æœæ˜¯GIFï¼Œå¤„ç†æ‰€æœ‰å¸§
        if getattr(img, "is_animated", False):
            frames = []
            for frame_idx in range(img.n_frames):
                img.seek(frame_idx)
                new_frame = img.copy()
                new_frame = new_frame.resize((new_width // 2, new_height // 2), Image.Resampling.LANCZOS)  # åŠ¨å›¾æŠ˜ä¸ŠæŠ˜
                frames.append(new_frame)

            # ä¿å­˜åˆ°ç¼“å†²åŒº
            frames[0].save(
                output_buffer,
                format="GIF",
                save_all=True,
                append_images=frames[1:],
                optimize=True,
                duration=img.info.get("duration", 100),
                loop=img.info.get("loop", 0),
            )
        else:
            # å¤„ç†é™æ€å›¾ç‰‡
            resized_img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)

            # ä¿å­˜åˆ°ç¼“å†²åŒºï¼Œä¿æŒåŸå§‹æ ¼å¼
            if img.format == "PNG" and img.mode in ("RGBA", "LA"):
                resized_img.save(output_buffer, format="PNG", optimize=True)
            else:
                resized_img.save(output_buffer, format="JPEG", quality=95, optimize=True)

        # è·å–å‹ç¼©åçš„æ•°æ®å¹¶è½¬æ¢ä¸ºbase64
        compressed_data = output_buffer.getvalue()
        logger.info(f"å‹ç¼©å›¾ç‰‡: {original_width}x{original_height} -> {new_width}x{new_height}")
        logger.info(f"å‹ç¼©å‰å¤§å°: {len(image_data) / 1024:.1f}KB, å‹ç¼©åå¤§å°: {len(compressed_data) / 1024:.1f}KB")

        return base64.b64encode(compressed_data).decode("utf-8")

    except Exception as e:
        logger.error(f"å‹ç¼©å›¾ç‰‡å¤±è´¥: {str(e)}")
        import traceback

        logger.error(traceback.format_exc())
        return base64_data
