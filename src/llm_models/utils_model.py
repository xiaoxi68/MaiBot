import re
from datetime import datetime
from typing import Tuple, Union
from src.common.logger import get_logger
import base64
from PIL import Image
import io
from src.common.database.database import db  # ç¡®ä¿ db è¢«å¯¼å…¥ç”¨äº create_tables
from src.common.database.database_model import LLMUsage  # å¯¼å…¥ LLMUsage æ¨¡å‹
from src.config.config import global_config
from rich.traceback import install

install(extra_lines=3)

logger = get_logger("model_utils")

# æ–°æ¶æ„å¯¼å…¥ - ä½¿ç”¨å»¶è¿Ÿå¯¼å…¥ä»¥æ”¯æŒfallbackæ¨¡å¼
try:
    from .model_manager import ModelManager
    from .model_client import ModelRequestHandler
    from .payload_content.message import MessageBuilder
    
    # ä¸åœ¨æ¨¡å—çº§åˆ«åˆå§‹åŒ–ModelManagerï¼Œå»¶è¿Ÿåˆ°å®é™…ä½¿ç”¨æ—¶
    ModelManager_class = ModelManager
    model_manager = None  # å»¶è¿Ÿåˆå§‹åŒ–
    
    # æ·»åŠ è¯·æ±‚å¤„ç†å™¨ç¼“å­˜ï¼Œé¿å…é‡å¤åˆ›å»º
    _request_handler_cache = {}  # æ ¼å¼: {(model_name, task_name): ModelRequestHandler}
    
    NEW_ARCHITECTURE_AVAILABLE = True
    logger.info("æ–°æ¶æ„æ¨¡å—å¯¼å…¥æˆåŠŸ")
except Exception as e:
    logger.warning(f"æ–°æ¶æ„ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨fallbackæ¨¡å¼: {str(e)}")
    ModelManager_class = None
    model_manager = None
    ModelRequestHandler = None
    MessageBuilder = None
    _request_handler_cache = {}
    NEW_ARCHITECTURE_AVAILABLE = False


class PayLoadTooLargeError(Exception):
    """è‡ªå®šä¹‰å¼‚å¸¸ç±»ï¼Œç”¨äºå¤„ç†è¯·æ±‚ä½“è¿‡å¤§é”™è¯¯"""

    def __init__(self, message: str):
        super().__init__(message)
        self.message = message

    def __str__(self):
        return "è¯·æ±‚ä½“è¿‡å¤§ï¼Œè¯·å°è¯•å‹ç¼©å›¾ç‰‡æˆ–å‡å°‘è¾“å…¥å†…å®¹ã€‚"


class RequestAbortException(Exception):
    """è‡ªå®šä¹‰å¼‚å¸¸ç±»ï¼Œç”¨äºå¤„ç†è¯·æ±‚ä¸­æ–­å¼‚å¸¸"""

    def __init__(self, message: str):
        super().__init__(message)
        self.message = message

    def __str__(self):
        return self.message


class PermissionDeniedException(Exception):
    """è‡ªå®šä¹‰å¼‚å¸¸ç±»ï¼Œç”¨äºå¤„ç†è®¿é—®æ‹’ç»çš„å¼‚å¸¸"""

    def __init__(self, message: str):
        super().__init__(message)
        self.message = message

    def __str__(self):
        return self.message


# å¸¸è§Error Code Mapping
error_code_mapping = {
    400: "å‚æ•°ä¸æ­£ç¡®",
    401: "API key é”™è¯¯ï¼Œè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ config/model_config.toml ä¸­çš„é…ç½®æ˜¯å¦æ­£ç¡®",
    402: "è´¦å·ä½™é¢ä¸è¶³",
    403: "éœ€è¦å®å,æˆ–ä½™é¢ä¸è¶³",
    404: "Not Found",
    429: "è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•",
    500: "æœåŠ¡å™¨å†…éƒ¨æ•…éšœ",
    503: "æœåŠ¡å™¨è´Ÿè½½è¿‡é«˜",
}




class LLMRequest:
    """
    é‡æ„åçš„LLMè¯·æ±‚ç±»ï¼ŒåŸºäºæ–°çš„model_managerå’Œmodel_clientæ¶æ„
    ä¿æŒå‘åå…¼å®¹çš„APIæ¥å£
    """
    
    # å®šä¹‰éœ€è¦è½¬æ¢çš„æ¨¡å‹åˆ—è¡¨ï¼Œä½œä¸ºç±»å˜é‡é¿å…é‡å¤
    MODELS_NEEDING_TRANSFORMATION = [
        "o1",
        "o1-2024-12-17",
        "o1-mini",
        "o1-mini-2024-09-12",
        "o1-preview",
        "o1-preview-2024-09-12",
        "o1-pro",
        "o1-pro-2025-03-19",
        "o3",
        "o3-2025-04-16",
        "o3-mini",
        "o3-mini-2025-01-31",
        "o4-mini",
        "o4-mini-2025-04-16",
    ]

    def __init__(self, model: dict, **kwargs):
        """
        åˆå§‹åŒ–LLMè¯·æ±‚å®ä¾‹
        Args:
            model: æ¨¡å‹é…ç½®å­—å…¸ï¼Œå…¼å®¹æ—§æ ¼å¼å’Œæ–°æ ¼å¼
            **kwargs: é¢å¤–å‚æ•°
        """
        logger.debug(f"ğŸ” [æ¨¡å‹åˆå§‹åŒ–] å¼€å§‹åˆå§‹åŒ–æ¨¡å‹: {model.get('model_name', model.get('name', 'Unknown'))}")
        logger.debug(f"ğŸ” [æ¨¡å‹åˆå§‹åŒ–] æ¨¡å‹é…ç½®: {model}")
        logger.debug(f"ğŸ” [æ¨¡å‹åˆå§‹åŒ–] é¢å¤–å‚æ•°: {kwargs}")
        
        # å…¼å®¹æ–°æ—§æ¨¡å‹é…ç½®æ ¼å¼
        # æ–°æ ¼å¼ä½¿ç”¨ model_nameï¼Œæ—§æ ¼å¼ä½¿ç”¨ name
        self.model_name: str = model.get("model_name", model.get("name", ""))
        # åœ¨æ–°æ¶æ„ä¸­ï¼Œproviderä¿¡æ¯ä»model_config.tomlè‡ªåŠ¨è·å–ï¼Œä¸éœ€è¦åœ¨è¿™é‡Œè®¾ç½®
        self.provider = model.get("provider", "")  # ä¿ç•™å…¼å®¹æ€§ï¼Œä½†åœ¨æ–°æ¶æ„ä¸­ä¸ä½¿ç”¨
        
        # ä»å…¨å±€é…ç½®ä¸­è·å–ä»»åŠ¡é…ç½®
        self.request_type = kwargs.pop("request_type", "default")
        
        # ç¡®å®šä½¿ç”¨å“ªä¸ªä»»åŠ¡é…ç½®
        task_name = self._determine_task_name(model)
        
        # åˆå§‹åŒ– request_handler
        self.request_handler = None
        
        # å°è¯•åˆå§‹åŒ–æ–°æ¶æ„
        if NEW_ARCHITECTURE_AVAILABLE and ModelManager_class is not None:
            try:
                # å»¶è¿Ÿåˆå§‹åŒ–ModelManager
                global model_manager, _request_handler_cache
                if model_manager is None:
                    from src.config.config import model_config
                    model_manager = ModelManager_class(model_config)
                    logger.debug("ğŸ” [æ¨¡å‹åˆå§‹åŒ–] ModelManagerå»¶è¿Ÿåˆå§‹åŒ–æˆåŠŸ")
                
                # æ„å»ºç¼“å­˜é”®
                cache_key = (self.model_name, task_name)
                
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„è¯·æ±‚å¤„ç†å™¨
                if cache_key in _request_handler_cache:
                    self.request_handler = _request_handler_cache[cache_key]
                    logger.debug(f"ğŸš€ [æ€§èƒ½ä¼˜åŒ–] ä»LLMRequestç¼“å­˜è·å–è¯·æ±‚å¤„ç†å™¨: {cache_key}")
                else:
                    # ä½¿ç”¨æ–°æ¶æ„è·å–æ¨¡å‹è¯·æ±‚å¤„ç†å™¨
                    self.request_handler = model_manager[task_name]
                    _request_handler_cache[cache_key] = self.request_handler
                    logger.debug(f"ğŸ”§ [æ€§èƒ½ä¼˜åŒ–] åˆ›å»ºå¹¶ç¼“å­˜LLMRequestè¯·æ±‚å¤„ç†å™¨: {cache_key}")
                
                logger.debug(f"ğŸ” [æ¨¡å‹åˆå§‹åŒ–] æˆåŠŸè·å–æ¨¡å‹è¯·æ±‚å¤„ç†å™¨ï¼Œä»»åŠ¡: {task_name}")
                self.use_new_architecture = True
            except Exception as e:
                logger.warning(f"æ— æ³•ä½¿ç”¨æ–°æ¶æ„ï¼Œä»»åŠ¡ {task_name} åˆå§‹åŒ–å¤±è´¥: {e}")
                logger.warning("å›é€€åˆ°å…¼å®¹æ¨¡å¼ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½å—é™")
                self.request_handler = None
                self.use_new_architecture = False
        else:
            logger.warning("æ–°æ¶æ„ä¸å¯ç”¨ï¼Œä½¿ç”¨å…¼å®¹æ¨¡å¼")
            logger.warning("å›é€€åˆ°å…¼å®¹æ¨¡å¼ï¼ŒæŸäº›åŠŸèƒ½å¯èƒ½å—é™")
            self.request_handler = None
            self.use_new_architecture = False
        
        # ä¿å­˜åŸå§‹å‚æ•°ç”¨äºå‘åå…¼å®¹
        self.params = kwargs
        
        # å…¼å®¹æ€§å±æ€§ï¼Œä»æ¨¡å‹é…ç½®ä¸­æå–
        # æ–°æ ¼å¼å’Œæ—§æ ¼å¼éƒ½æ”¯æŒ
        self.enable_thinking = model.get("enable_thinking", False)
        self.temp = model.get("temperature", model.get("temp", 0.7))  # æ–°æ ¼å¼ç”¨temperatureï¼Œæ—§æ ¼å¼ç”¨temp
        self.thinking_budget = model.get("thinking_budget", 4096)
        self.stream = model.get("stream", False)
        self.pri_in = model.get("pri_in", 0)
        self.pri_out = model.get("pri_out", 0)
        self.max_tokens = model.get("max_tokens", global_config.model.model_max_output_length)
        
        # è®°å½•é…ç½®æ–‡ä»¶ä¸­å£°æ˜äº†å“ªäº›å‚æ•°ï¼ˆä¸ç®¡å€¼æ˜¯ä»€ä¹ˆï¼‰
        self.has_enable_thinking = "enable_thinking" in model
        self.has_thinking_budget = "thinking_budget" in model
        self.pri_out = model.get("pri_out", 0)
        self.max_tokens = model.get("max_tokens", global_config.model.model_max_output_length)
        
        # è®°å½•é…ç½®æ–‡ä»¶ä¸­å£°æ˜äº†å“ªäº›å‚æ•°ï¼ˆä¸ç®¡å€¼æ˜¯ä»€ä¹ˆï¼‰
        self.has_enable_thinking = "enable_thinking" in model
        self.has_thinking_budget = "thinking_budget" in model
        
        logger.debug("ğŸ” [æ¨¡å‹åˆå§‹åŒ–] æ¨¡å‹å‚æ•°è®¾ç½®å®Œæˆ:")
        logger.debug(f"   - model_name: {self.model_name}")
        logger.debug(f"   - provider: {self.provider}")
        logger.debug(f"   - has_enable_thinking: {self.has_enable_thinking}")
        logger.debug(f"   - enable_thinking: {self.enable_thinking}")
        logger.debug(f"   - has_thinking_budget: {self.has_thinking_budget}")
        logger.debug(f"   - thinking_budget: {self.thinking_budget}")
        logger.debug(f"   - temp: {self.temp}")
        logger.debug(f"   - stream: {self.stream}")
        logger.debug(f"   - max_tokens: {self.max_tokens}")
        logger.debug(f"   - use_new_architecture: {self.use_new_architecture}")

        # è·å–æ•°æ®åº“å®ä¾‹
        self._init_database()
        
        logger.debug(f"ğŸ” [æ¨¡å‹åˆå§‹åŒ–] åˆå§‹åŒ–å®Œæˆï¼Œrequest_type: {self.request_type}")

    def _determine_task_name(self, model: dict) -> str:
        """
        æ ¹æ®æ¨¡å‹é…ç½®ç¡®å®šä»»åŠ¡åç§°
        ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­æ˜ç¡®å®šä¹‰çš„ä»»åŠ¡ç±»å‹ï¼Œé¿å…åŸºäºæ¨¡å‹åç§°çš„è„†å¼±æ¨æ–­
        
        Args:
            model: æ¨¡å‹é…ç½®å­—å…¸
        Returns:
            ä»»åŠ¡åç§°
        """
        # æ–¹æ³•1: ä¼˜å…ˆä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­æ˜ç¡®å®šä¹‰çš„ task_type å­—æ®µ
        if "task_type" in model:
            task_type = model["task_type"]
            logger.debug(f"ğŸ¯ [ä»»åŠ¡ç¡®å®š] ä½¿ç”¨é…ç½®ä¸­çš„ task_type: {task_type}")
            return task_type
        
        # æ–¹æ³•2: ä½¿ç”¨ capabilities å­—æ®µæ¥æ¨æ–­ä¸»è¦ä»»åŠ¡ç±»å‹
        if "capabilities" in model:
            capabilities = model["capabilities"]
            if isinstance(capabilities, list):
                # æŒ‰ä¼˜å…ˆçº§é¡ºåºæ£€æŸ¥èƒ½åŠ›
                if "vision" in capabilities:
                    logger.debug(f"ğŸ¯ [ä»»åŠ¡ç¡®å®š] ä» capabilities {capabilities} æ¨æ–­ä¸º: vision")
                    return "vision"
                elif "embedding" in capabilities:
                    logger.debug(f"ğŸ¯ [ä»»åŠ¡ç¡®å®š] ä» capabilities {capabilities} æ¨æ–­ä¸º: embedding")
                    return "embedding"
                elif "speech" in capabilities:
                    logger.debug(f"ğŸ¯ [ä»»åŠ¡ç¡®å®š] ä» capabilities {capabilities} æ¨æ–­ä¸º: speech")
                    return "speech"
                elif "text" in capabilities:
                    # å¦‚æœåªæœ‰æ–‡æœ¬èƒ½åŠ›ï¼Œåˆ™æ ¹æ®request_typeç»†åˆ†
                    task = "llm_reasoning" if self.request_type == "reasoning" else "llm_normal"
                    logger.debug(f"ğŸ¯ [ä»»åŠ¡ç¡®å®š] ä» capabilities {capabilities} å’Œ request_type {self.request_type} æ¨æ–­ä¸º: {task}")
                    return task
        
        # æ–¹æ³•3: å‘åå…¼å®¹ - åŸºäºæ¨¡å‹åç§°çš„å…³é”®å­—æ¨æ–­ï¼ˆä¸æ¨èä½†ä¿ç•™å…¼å®¹æ€§ï¼‰
        model_name = model.get("model_name", model.get("name", ""))
        logger.warning(f"âš ï¸ [ä»»åŠ¡ç¡®å®š] é…ç½®ä¸­æœªæ‰¾åˆ° task_type æˆ– capabilitiesï¼Œå›é€€åˆ°åŸºäºæ¨¡å‹åç§°çš„æ¨æ–­: {model_name}")
        logger.warning("âš ï¸ [å»ºè®®] è¯·åœ¨ model_config.toml ä¸­ä¸ºæ¨¡å‹æ·»åŠ æ˜ç¡®çš„ task_type æˆ– capabilities å­—æ®µ")
        
        # ä¿ç•™åŸæœ‰çš„å…³é”®å­—åŒ¹é…é€»è¾‘ä½œä¸ºfallback
        if any(keyword in model_name.lower() for keyword in ["vlm", "vision", "gpt-4o", "claude", "vl-"]):
            logger.debug(f"ğŸ¯ [ä»»åŠ¡ç¡®å®š] ä»æ¨¡å‹åç§° {model_name} æ¨æ–­ä¸º: vision")
            return "vision"
        elif any(keyword in model_name.lower() for keyword in ["embed", "text-embedding", "bge-"]):
            logger.debug(f"ğŸ¯ [ä»»åŠ¡ç¡®å®š] ä»æ¨¡å‹åç§° {model_name} æ¨æ–­ä¸º: embedding")
            return "embedding" 
        elif any(keyword in model_name.lower() for keyword in ["whisper", "speech", "voice"]):
            logger.debug(f"ğŸ¯ [ä»»åŠ¡ç¡®å®š] ä»æ¨¡å‹åç§° {model_name} æ¨æ–­ä¸º: speech")
            return "speech"
        else:
            # æ ¹æ®request_typeç¡®å®šï¼Œæ˜ å°„åˆ°é…ç½®æ–‡ä»¶ä¸­å®šä¹‰çš„ä»»åŠ¡
            task = "llm_reasoning" if self.request_type == "reasoning" else "llm_normal"
            logger.debug(f"ğŸ¯ [ä»»åŠ¡ç¡®å®š] ä» request_type {self.request_type} æ¨æ–­ä¸º: {task}")
            return task

    @staticmethod
    def _init_database():
        """åˆå§‹åŒ–æ•°æ®åº“é›†åˆ"""
        try:
            # ä½¿ç”¨ Peewee åˆ›å»ºè¡¨ï¼Œsafe=True è¡¨ç¤ºå¦‚æœè¡¨å·²å­˜åœ¨åˆ™ä¸ä¼šæŠ›å‡ºé”™è¯¯
            db.create_tables([LLMUsage], safe=True)
            # logger.debug("LLMUsage è¡¨å·²åˆå§‹åŒ–/ç¡®ä¿å­˜åœ¨ã€‚")
        except Exception as e:
            logger.error(f"åˆ›å»º LLMUsage è¡¨å¤±è´¥: {str(e)}")

    def _record_usage(
        self,
        prompt_tokens: int,
        completion_tokens: int,
        total_tokens: int,
        user_id: str = "system",
        request_type: str | None = None,
        endpoint: str = "/chat/completions",
    ):
        """è®°å½•æ¨¡å‹ä½¿ç”¨æƒ…å†µåˆ°æ•°æ®åº“
        Args:
            prompt_tokens: è¾“å…¥tokenæ•°
            completion_tokens: è¾“å‡ºtokenæ•°
            total_tokens: æ€»tokenæ•°
            user_id: ç”¨æˆ·IDï¼Œé»˜è®¤ä¸ºsystem
            request_type: è¯·æ±‚ç±»å‹
            endpoint: APIç«¯ç‚¹
        """
        # å¦‚æœ request_type ä¸º Noneï¼Œåˆ™ä½¿ç”¨å®ä¾‹å˜é‡ä¸­çš„å€¼
        if request_type is None:
            request_type = self.request_type

        try:
            # ä½¿ç”¨ Peewee æ¨¡å‹åˆ›å»ºè®°å½•
            LLMUsage.create(
                model_name=self.model_name,
                user_id=user_id,
                request_type=request_type,
                endpoint=endpoint,
                prompt_tokens=prompt_tokens,
                completion_tokens=completion_tokens,
                total_tokens=total_tokens,
                cost=self._calculate_cost(prompt_tokens, completion_tokens),
                status="success",
                timestamp=datetime.now(),  # Peewee ä¼šå¤„ç† DateTimeField
            )
            logger.debug(
                f"Tokenä½¿ç”¨æƒ…å†µ - æ¨¡å‹: {self.model_name}, "
                f"ç”¨æˆ·: {user_id}, ç±»å‹: {request_type}, "
                f"æç¤ºè¯: {prompt_tokens}, å®Œæˆ: {completion_tokens}, "
                f"æ€»è®¡: {total_tokens}"
            )
        except Exception as e:
            logger.error(f"è®°å½•tokenä½¿ç”¨æƒ…å†µå¤±è´¥: {str(e)}")

    def _calculate_cost(self, prompt_tokens: int, completion_tokens: int) -> float:
        """è®¡ç®—APIè°ƒç”¨æˆæœ¬
        ä½¿ç”¨æ¨¡å‹çš„pri_inå’Œpri_outä»·æ ¼è®¡ç®—è¾“å…¥å’Œè¾“å‡ºçš„æˆæœ¬

        Args:
            prompt_tokens: è¾“å…¥tokenæ•°é‡
            completion_tokens: è¾“å‡ºtokenæ•°é‡

        Returns:
            float: æ€»æˆæœ¬ï¼ˆå…ƒï¼‰
        """
        # ä½¿ç”¨æ¨¡å‹çš„pri_inå’Œpri_outè®¡ç®—æˆæœ¬
        input_cost = (prompt_tokens / 1000000) * self.pri_in
        output_cost = (completion_tokens / 1000000) * self.pri_out
        return round(input_cost + output_cost, 6)

    @staticmethod
    def _extract_reasoning(content: str) -> Tuple[str, str]:
        """CoTæ€ç»´é“¾æå–"""
        match = re.search(r"(?:<think>)?(.*?)</think>", content, re.DOTALL)
        content = re.sub(r"(?:<think>)?.*?</think>", "", content, flags=re.DOTALL, count=1).strip()
        reasoning = match[1].strip() if match else ""
        return content, reasoning

    # === ä¸»è¦APIæ–¹æ³• ===
    # è¿™äº›æ–¹æ³•æä¾›ä¸æ–°æ¶æ„çš„æ¡¥æ¥

    async def generate_response_for_image(self, prompt: str, image_base64: str, image_format: str) -> Tuple:
        """
        æ ¹æ®è¾“å…¥çš„æç¤ºå’Œå›¾ç‰‡ç”Ÿæˆæ¨¡å‹çš„å¼‚æ­¥å“åº”
        ä½¿ç”¨æ–°æ¶æ„çš„æ¨¡å‹è¯·æ±‚å¤„ç†å™¨
        """
        if not self.use_new_architecture:
            raise RuntimeError(
                f"æ¨¡å‹ {self.model_name} æ— æ³•ä½¿ç”¨æ–°æ¶æ„ï¼Œè¯·æ£€æŸ¥ config/model_config.toml ä¸­çš„ API é…ç½®ã€‚"
            )
        
        if self.request_handler is None:
            raise RuntimeError(
                f"æ¨¡å‹ {self.model_name} è¯·æ±‚å¤„ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•å¤„ç†å›¾ç‰‡è¯·æ±‚"
            )
        
        if MessageBuilder is None:
            raise RuntimeError("MessageBuilderä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥æ–°æ¶æ„é…ç½®")
            
        try:
            # æ„å»ºåŒ…å«å›¾ç‰‡çš„æ¶ˆæ¯
            message_builder = MessageBuilder()
            message_builder.add_text_content(prompt).add_image_content(
                image_format=image_format,
                image_base64=image_base64
            )
            messages = [message_builder.build()]
            
            # ä½¿ç”¨æ–°æ¶æ„å‘é€è¯·æ±‚ï¼ˆåªä¼ é€’æ”¯æŒçš„å‚æ•°ï¼‰
            response = await self.request_handler.get_response(  # type: ignore
                messages=messages,
                tool_options=None,
                response_format=None
            )
            
            # æ–°æ¶æ„è¿”å›çš„æ˜¯ APIResponse å¯¹è±¡ï¼Œç›´æ¥æå–å†…å®¹
            content = response.content or ""
            reasoning_content = response.reasoning_content or ""
            tool_calls = response.tool_calls
            
            # ä»å†…å®¹ä¸­æå–<think>æ ‡ç­¾çš„æ¨ç†å†…å®¹ï¼ˆå‘åå…¼å®¹ï¼‰
            if not reasoning_content and content:
                content, extracted_reasoning = self._extract_reasoning(content)
                reasoning_content = extracted_reasoning
            
            # è®°å½•tokenä½¿ç”¨æƒ…å†µ
            if response.usage:
                self._record_usage(
                    prompt_tokens=response.usage.prompt_tokens or 0,
                    completion_tokens=response.usage.completion_tokens or 0,
                    total_tokens=response.usage.total_tokens or 0,
                    user_id="system",
                    request_type=self.request_type,
                    endpoint="/chat/completions"
                )
            
            # è¿”å›æ ¼å¼å…¼å®¹æ—§ç‰ˆæœ¬
            if tool_calls:
                return content, reasoning_content, tool_calls
            else:
                return content, reasoning_content
            
        except Exception as e:
            logger.error(f"æ¨¡å‹ {self.model_name} å›¾ç‰‡å“åº”ç”Ÿæˆå¤±è´¥: {str(e)}")
            # å‘åå…¼å®¹çš„å¼‚å¸¸å¤„ç†
            if "401" in str(e) or "API key" in str(e):
                raise RuntimeError("API key é”™è¯¯ï¼Œè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ config/model_config.toml ä¸­çš„ API key é…ç½®æ˜¯å¦æ­£ç¡®") from e
            elif "429" in str(e):
                raise RuntimeError("è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•") from e
            elif "500" in str(e) or "503" in str(e):
                raise RuntimeError("æœåŠ¡å™¨è´Ÿè½½è¿‡é«˜ï¼Œæ¨¡å‹å›å¤å¤±è´¥QAQ") from e
            else:
                raise RuntimeError(f"æ¨¡å‹ {self.model_name} APIè¯·æ±‚å¤±è´¥: {str(e)}") from e

    async def generate_response_for_voice(self, voice_bytes: bytes) -> Tuple:
        """
        æ ¹æ®è¾“å…¥çš„è¯­éŸ³æ–‡ä»¶ç”Ÿæˆæ¨¡å‹çš„å¼‚æ­¥å“åº”
        ä½¿ç”¨æ–°æ¶æ„çš„æ¨¡å‹è¯·æ±‚å¤„ç†å™¨
        """
        if not self.use_new_architecture:
            raise RuntimeError(
                f"æ¨¡å‹ {self.model_name} æ— æ³•ä½¿ç”¨æ–°æ¶æ„ï¼Œè¯·æ£€æŸ¥ config/model_config.toml ä¸­çš„ API é…ç½®ã€‚"
            )
            
        if self.request_handler is None:
            raise RuntimeError(
                f"æ¨¡å‹ {self.model_name} è¯·æ±‚å¤„ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•å¤„ç†è¯­éŸ³è¯·æ±‚"
            )
            
        try:
            # æ„å»ºè¯­éŸ³è¯†åˆ«è¯·æ±‚å‚æ•°
            # æ³¨æ„ï¼šæ–°æ¶æ„ä¸­çš„è¯­éŸ³è¯†åˆ«å¯èƒ½ä½¿ç”¨ä¸åŒçš„æ–¹æ³•
            # è¿™é‡Œå…ˆä½¿ç”¨get_responseæ–¹æ³•ï¼Œå¯èƒ½éœ€è¦æ ¹æ®å®é™…APIè°ƒæ•´
            response = await self.request_handler.get_response(  # type: ignore
                messages=[],  # è¯­éŸ³è¯†åˆ«å¯èƒ½ä¸éœ€è¦æ¶ˆæ¯
                tool_options=None
            )
            
            # æ–°æ¶æ„è¿”å›çš„æ˜¯ APIResponse å¯¹è±¡ï¼Œç›´æ¥æå–æ–‡æœ¬å†…å®¹
            return (response.content,) if response.content else ("",)
            
        except Exception as e:
            logger.error(f"æ¨¡å‹ {self.model_name} è¯­éŸ³è¯†åˆ«å¤±è´¥: {str(e)}")
            # å‘åå…¼å®¹çš„å¼‚å¸¸å¤„ç†
            if "401" in str(e) or "API key" in str(e):
                raise RuntimeError("API key é”™è¯¯ï¼Œè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ config/model_config.toml ä¸­çš„ API key é…ç½®æ˜¯å¦æ­£ç¡®") from e
            elif "429" in str(e):
                raise RuntimeError("è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•") from e
            elif "500" in str(e) or "503" in str(e):
                raise RuntimeError("æœåŠ¡å™¨è´Ÿè½½è¿‡é«˜ï¼Œæ¨¡å‹å›å¤å¤±è´¥QAQ") from e
            else:
                raise RuntimeError(f"æ¨¡å‹ {self.model_name} APIè¯·æ±‚å¤±è´¥: {str(e)}") from e

    async def generate_response_async(self, prompt: str, **kwargs) -> Union[str, Tuple]:
        """
        å¼‚æ­¥æ–¹å¼æ ¹æ®è¾“å…¥çš„æç¤ºç”Ÿæˆæ¨¡å‹çš„å“åº”
        ä½¿ç”¨æ–°æ¶æ„çš„æ¨¡å‹è¯·æ±‚å¤„ç†å™¨ï¼Œå¦‚æ— æ³•ä½¿ç”¨åˆ™æŠ›å‡ºé”™è¯¯
        """
        if not self.use_new_architecture:
            raise RuntimeError(
                f"æ¨¡å‹ {self.model_name} æ— æ³•ä½¿ç”¨æ–°æ¶æ„ï¼Œè¯·æ£€æŸ¥ config/model_config.toml ä¸­çš„ API é…ç½®ã€‚"
            )
        
        if self.request_handler is None:
            raise RuntimeError(
                f"æ¨¡å‹ {self.model_name} è¯·æ±‚å¤„ç†å™¨æœªåˆå§‹åŒ–ï¼Œæ— æ³•ç”Ÿæˆå“åº”"
            )
        
        if MessageBuilder is None:
            raise RuntimeError("MessageBuilderä¸å¯ç”¨ï¼Œè¯·æ£€æŸ¥æ–°æ¶æ„é…ç½®")
        
        try:
            # æ„å»ºæ¶ˆæ¯
            message_builder = MessageBuilder()
            message_builder.add_text_content(prompt)
            messages = [message_builder.build()]
            
            # ä½¿ç”¨æ–°æ¶æ„å‘é€è¯·æ±‚ï¼ˆåªä¼ é€’æ”¯æŒçš„å‚æ•°ï¼‰
            response = await self.request_handler.get_response(  # type: ignore
                messages=messages,
                tool_options=None,
                response_format=None
            )
            
            # æ–°æ¶æ„è¿”å›çš„æ˜¯ APIResponse å¯¹è±¡ï¼Œç›´æ¥æå–å†…å®¹
            content = response.content or ""
            reasoning_content = response.reasoning_content or ""
            tool_calls = response.tool_calls
            
            # ä»å†…å®¹ä¸­æå–<think>æ ‡ç­¾çš„æ¨ç†å†…å®¹ï¼ˆå‘åå…¼å®¹ï¼‰
            if not reasoning_content and content:
                content, extracted_reasoning = self._extract_reasoning(content)
                reasoning_content = extracted_reasoning
            
            # è®°å½•tokenä½¿ç”¨æƒ…å†µ
            if response.usage:
                self._record_usage(
                    prompt_tokens=response.usage.prompt_tokens or 0,
                    completion_tokens=response.usage.completion_tokens or 0,
                    total_tokens=response.usage.total_tokens or 0,
                    user_id="system",
                    request_type=self.request_type,
                    endpoint="/chat/completions"
                )
            
            # è¿”å›æ ¼å¼å…¼å®¹æ—§ç‰ˆæœ¬
            if tool_calls:
                return content, (reasoning_content, self.model_name, tool_calls)
            else:
                return content, (reasoning_content, self.model_name)
            
        except Exception as e:
            logger.error(f"æ¨¡å‹ {self.model_name} ç”Ÿæˆå“åº”å¤±è´¥: {str(e)}")
            # å‘åå…¼å®¹çš„å¼‚å¸¸å¤„ç†
            if "401" in str(e) or "API key" in str(e):
                raise RuntimeError("API key é”™è¯¯ï¼Œè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ config/model_config.toml ä¸­çš„ API key é…ç½®æ˜¯å¦æ­£ç¡®") from e
            elif "429" in str(e):
                raise RuntimeError("è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•") from e
            elif "500" in str(e) or "503" in str(e):
                raise RuntimeError("æœåŠ¡å™¨è´Ÿè½½è¿‡é«˜ï¼Œæ¨¡å‹å›å¤å¤±è´¥QAQ") from e
            else:
                raise RuntimeError(f"æ¨¡å‹ {self.model_name} APIè¯·æ±‚å¤±è´¥: {str(e)}") from e

    async def get_embedding(self, text: str) -> Union[list, None]:
        """
        å¼‚æ­¥æ–¹æ³•ï¼šè·å–æ–‡æœ¬çš„embeddingå‘é‡
        ä½¿ç”¨æ–°æ¶æ„çš„æ¨¡å‹è¯·æ±‚å¤„ç†å™¨

        Args:
            text: éœ€è¦è·å–embeddingçš„æ–‡æœ¬

        Returns:
            list: embeddingå‘é‡ï¼Œå¦‚æœå¤±è´¥åˆ™è¿”å›None
        """
        if not text:
            logger.debug("è¯¥æ¶ˆæ¯æ²¡æœ‰é•¿åº¦ï¼Œä¸å†å‘é€è·å–embeddingå‘é‡çš„è¯·æ±‚")
            return None

        if not self.use_new_architecture:
            logger.warning(f"æ¨¡å‹ {self.model_name} æ— æ³•ä½¿ç”¨æ–°æ¶æ„ï¼Œembeddingè¯·æ±‚å°†è¢«è·³è¿‡")
            return None

        if self.request_handler is None:
            logger.warning(f"æ¨¡å‹ {self.model_name} è¯·æ±‚å¤„ç†å™¨æœªåˆå§‹åŒ–ï¼Œembeddingè¯·æ±‚å°†è¢«è·³è¿‡")
            return None

        try:
            # æ„å»ºembeddingè¯·æ±‚å‚æ•°
            # ä½¿ç”¨æ–°æ¶æ„çš„get_embeddingæ–¹æ³•
            response = await self.request_handler.get_embedding(text)  # type: ignore
            
            # æ–°æ¶æ„è¿”å›çš„æ˜¯ APIResponse å¯¹è±¡ï¼Œç›´æ¥æå–embedding
            if response.embedding:
                embedding = response.embedding
                
                # è®°å½•tokenä½¿ç”¨æƒ…å†µ
                if response.usage:
                    self._record_usage(
                        prompt_tokens=response.usage.prompt_tokens or 0,
                        completion_tokens=response.usage.completion_tokens or 0,
                        total_tokens=response.usage.total_tokens or 0,
                        user_id="system",
                        request_type=self.request_type,
                        endpoint="/embeddings"
                    )
                
                return embedding
            else:
                logger.warning(f"æ¨¡å‹ {self.model_name} è¿”å›çš„embeddingå“åº”ä¸ºç©º")
                return None
            
        except Exception as e:
            logger.error(f"æ¨¡å‹ {self.model_name} è·å–embeddingå¤±è´¥: {str(e)}")
            # å‘åå…¼å®¹çš„å¼‚å¸¸å¤„ç†
            if "401" in str(e) or "API key" in str(e):
                raise RuntimeError("API key é”™è¯¯ï¼Œè®¤è¯å¤±è´¥ï¼Œè¯·æ£€æŸ¥ config/model_config.toml ä¸­çš„ API key é…ç½®æ˜¯å¦æ­£ç¡®") from e
            elif "429" in str(e):
                raise RuntimeError("è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè¯·ç¨åå†è¯•") from e
            elif "500" in str(e) or "503" in str(e):
                raise RuntimeError("æœåŠ¡å™¨è´Ÿè½½è¿‡é«˜ï¼Œæ¨¡å‹å›å¤å¤±è´¥QAQ") from e
            else:
                logger.warning(f"æ¨¡å‹ {self.model_name} embeddingè¯·æ±‚å¤±è´¥ï¼Œè¿”å›None: {str(e)}")
                return None


def compress_base64_image_by_scale(base64_data: str, target_size: int = int(0.8 * 1024 * 1024)) -> str:
    """å‹ç¼©base64æ ¼å¼çš„å›¾ç‰‡åˆ°æŒ‡å®šå¤§å°
    Args:
        base64_data: base64ç¼–ç çš„å›¾ç‰‡æ•°æ®
        target_size: ç›®æ ‡æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰ï¼Œé»˜è®¤0.8MB
    Returns:
        str: å‹ç¼©åçš„base64å›¾ç‰‡æ•°æ®
    """
    try:
        # å°†base64è½¬æ¢ä¸ºå­—èŠ‚æ•°æ®
        # ç¡®ä¿base64å­—ç¬¦ä¸²åªåŒ…å«ASCIIå­—ç¬¦
        if isinstance(base64_data, str):
            base64_data = base64_data.encode("ascii", errors="ignore").decode("ascii")
        image_data = base64.b64decode(base64_data)

        # å¦‚æœå·²ç»å°äºç›®æ ‡å¤§å°ï¼Œç›´æ¥è¿”å›åŸå›¾
        if len(image_data) <= 2 * 1024 * 1024:
            return base64_data

        # å°†å­—èŠ‚æ•°æ®è½¬æ¢ä¸ºå›¾ç‰‡å¯¹è±¡
        img = Image.open(io.BytesIO(image_data))

        # è·å–åŸå§‹å°ºå¯¸
        original_width, original_height = img.size

        # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
        scale = min(1.0, (target_size / len(image_data)) ** 0.5)

        # è®¡ç®—æ–°çš„å°ºå¯¸
        new_width = int(original_width * scale)
        new_height = int(original_height * scale)

        # åˆ›å»ºå†…å­˜ç¼“å†²åŒº
        output_buffer = io.BytesIO()

        # å¦‚æœæ˜¯GIFï¼Œå¤„ç†æ‰€æœ‰å¸§
        if getattr(img, "is_animated", False):
            frames = []
            n_frames = getattr(img, 'n_frames', 1)
            for frame_idx in range(n_frames):
                img.seek(frame_idx)
                new_frame = img.copy()
                new_frame = new_frame.resize((new_width // 2, new_height // 2), Image.Resampling.LANCZOS)  # åŠ¨å›¾æŠ˜ä¸ŠæŠ˜
                frames.append(new_frame)

            # ä¿å­˜åˆ°ç¼“å†²åŒº
            frames[0].save(
                output_buffer,
                format="GIF",
                save_all=True,
                append_images=frames[1:],
                optimize=True,
                duration=img.info.get("duration", 100),
                loop=img.info.get("loop", 0),
            )
        else:
            # å¤„ç†é™æ€å›¾ç‰‡
            resized_img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)

            # ä¿å­˜åˆ°ç¼“å†²åŒºï¼Œä¿æŒåŸå§‹æ ¼å¼
            if img.format == "PNG" and img.mode in ("RGBA", "LA"):
                resized_img.save(output_buffer, format="PNG", optimize=True)
            else:
                resized_img.save(output_buffer, format="JPEG", quality=95, optimize=True)

        # è·å–å‹ç¼©åçš„æ•°æ®å¹¶è½¬æ¢ä¸ºbase64
        compressed_data = output_buffer.getvalue()
        logger.info(f"å‹ç¼©å›¾ç‰‡: {original_width}x{original_height} -> {new_width}x{new_height}")
        logger.info(f"å‹ç¼©å‰å¤§å°: {len(image_data) / 1024:.1f}KB, å‹ç¼©åå¤§å°: {len(compressed_data) / 1024:.1f}KB")

        return base64.b64encode(compressed_data).decode("utf-8")

    except Exception as e:
        logger.error(f"å‹ç¼©å›¾ç‰‡å¤±è´¥: {str(e)}")
        import traceback

        logger.error(traceback.format_exc())
        return base64_data
